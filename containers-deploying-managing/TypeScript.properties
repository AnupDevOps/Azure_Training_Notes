Course Overview
[Autogenerated] Hi, everyone. My name's Mark Heath, and welcome to my course, Microsoft Azure Developer: Deploying and Managing Containers. I'm a Microsoft Azure MVP and I work as a cloud architect at NICE Systems. Docker containers are a fantastic way to package and deploy your applications, and Azure offers several services that make hosting those containers in the cloud really easy. In this course, I'm going to introduce you to everything you need to know to containerize your applications and get them up and running in the cloud.  We'll start by seeing how to run Docker containers locally and how to create our own images using Dockerfiles, and then we'll see how we can host those images in Azure Container Registry. Next, we'll explore four key Azure services that can run your containers, starting with Azure Container Instances, which offers serverless container hosting, and then Azure Web App for Containers, a great platform for hosting web applications, before moving on to explore two extremely powerful container orchestrators, Azure Kubernetes Service and Azure Service Fabric, both of which are ideal for hosting containerized microservice applications. By the end of this course, you'll have a solid understanding of the tools and services at your disposal for deploying containers to Azure, and you'll be able to follow along with this course, even if you've not used Docker or Azure before. But if you want to try out the demo scenarios yourself, it will help if you've already got an Azure subscription and know how to use the Azure portal.  I hope you'll join me on this journey to learn all about hosting containers with the Microsoft Azure Developer: Deploying and Managing Containers course, at Pluralsight.

Introducing Containers on Azure
Course Introduction
[Autogenerated] Hi, my name's Mark Heath. And welcome to this course Microsoft Azure developer. Deploy and manage containers in this course will learn about several different ways to host containers in Azure. And we'll start off in this module by looking at the big picture what our doctor containers and what benefits do they bring. And what are our options for deploying them in Azure In the next module will learn about how we can run containers locally with Dr and I'll introduce you to some of the most important doctor commands. And then we'll learn how you can create your own doctor images and published them to a container registry hosted in a. Sure and then we'll move on to look in particular at four different ways of hosting containers in Azure. These are as your container instances, which offers a serverless way of hosting containers then as your Web caps for containers, which is a service particularly suited to hosting containers for Web applications. And then we'll look as your service fabric, which is a powerful orchestrator ideal for hosting micro service is, and it uses the same highly scaleable platform that powers many of Microsoft's own ashes service is, and finally, as a kubernetes service or a ks. You might know that Kubernetes is one of the most popular container orchestration platforms, and a ks is a managed instance of kubernetes that makes it really simple to get started with. And we'll finish off by looking at how we can ensure that we follow best practices for keeping our containers secure. And although you're welcome to follow along from start to finish, I've also tried to design this course so that you can jump in at any point and just watch the material most relevant to you. Maybe you already know how to use doctor and create daca files, and if so, that's fine. Feel free to dive into the later modules to learn about the azure service is that you're interested in.

Docker Basics
Let's start off by learning a few Docker basics. Two of the most important concepts are images and containers. A Docker image is simply a file in which you package an application along with all of its dependencies. And then when we want to run that application, we create a container, which is an instance of that image that can be run on a Docker host. Now the benefit of doing this is that if I've got a Docker image, then I know that it will work exactly the same on any Docker host, whether that's my local development machine, or a virtual machine in Azure, or even an on-premises data center. And that's because all the dependencies of my application are built into the image. And Docker images are created out of layers. So if I had an Elasticsearch image, that might be based on top of a Java image, which in turn might be based on an Ubuntu image. And this layering approach actually allows for much more efficient use of disk space. The instructions to build a Docker image are stored in a Dockerfile, and you can reuse images that other people have built or write your own Dockerfiles, which allows you to create Docker images for your own applications. When you create a Docker image, you give it a name such as markheath/myapp, and you can also give it a tag, 1.4 in this example. And this allows you to create multiple images for different versions of the same application. Docker images can be published to a container registry, allowing them to be shared with other developers. These registries can be public, which would be appropriate for sharing images of open-source software, or private, which you'd use for your own custom applications. Docker Hub is an example of a public container registry that contains thousands of images for popular open-source applications that you can easily use, and Azure Container Registry is a great choice for securely storing your private images if you plan to host them in Azure. So if you're building a microservices application, for example, you might publish each of your microservices as a container image into Azure Container Registry. Now, for a Docker container to run, it obviously needs access to some of the resources on the Docker host. So the Docker host provides CPU and memory for the container instance, but it can constrain these if you want to limit how much it can use. But Docker host can also publish ports, allowing network access into a container instance. And the Docker host also provides disk access, but this is tightly locked down. A Docker container can't just go looking around anywhere on the disk of the host. It can only see content of the image and any other storage that you've explicitly chosen to share with this container. Containers can be stopped and restarted, and you can simply delete them when you're done with them. You can even create multiple containers from a single image. So if you want to run several instances of an application at once, that's really easy to achieve. Now, what happens if a container wants to store some data? Well, let's imagine we have a Docker image of Elasticsearch, and we create two containers based off of that image. Now it might seem that we've got three copies on disk of each of these layers, but actually we don't. Layers are read-only, which means that they can be safely shared, and this is one of the things that makes Docker containers very efficient in terms of disk space. But suppose in container one I wanted to create an Elasticsearch index. Where does the data get stored? Well, by default, each container gets its own writeable layer that it can persist data into, and this sits on top of the read-only base layers, so each container could store different data independently of each other. But this is not a recommended pattern. Instead, containers should use volumes to persist data. A volume is an independent storage location, such as a folder on the Docker host or an Azure file share, and the container is granted access to that location by mounting it as a volume. When you mount a volume, you can specify a path inside the container that the volume should be mounted to. So, for example, with a MySQL container, you might mount a volume to /var/lib/mysql, which is the default location for databases to be saved to. And so any databases you created would end up being stored in the volume and not in the container. And this approach allows the lifetime of the data to be independent of the lifetime of the container, and that's really important for upgrades. Suppose I'm running Elasticsearch version 6 in a container with the index data stored in a volume. And let's imagine that a new version, Elasticsearch 6.1, comes out, and I want to upgrade to it. Now, in theory, I could take my running container and just upgrade it to Elasticsearch 6.1, which will actually end up meaning that I've got another layer of changes inside the container itself. But instead, a much better practice is to simply delete the container because it doesn't hold any data. That's safely stored in the volume. And then I create a new container based on an image of Elasticsearch 6.1 and mount the same volume to that container. Now, we've said that the Docker host can allow containers to publish ports so that they can be accessed over the network. And so here I've got a container running Elasticsearch, which exposes port 9200. I can publish this port externally to allow incoming traffic into my container on port 9200. But if I wanted to run a second container based off the same image, then I can't reuse that same port number. But Docker supports port mapping, so I could map port 9201 externally to 9200 in my second container, and this makes it really easy to run multiple containers on the same host even if they have conflicting port requirements. Containers can also have their own environment variables, so two containers both running the same Docker image could behave differently because they've got different environment variables configured, as we can see in this example. It's also worth knowing that there are two different types of containers, Linux and Windows. Docker started life as a Linux technology, and the majority of Docker images that you come across are likely to be Linux images. However, starting with Windows Server 2016, it's now possible to have Windows images, and this allows you to bring all the benefits of containers to applications that target Windows, for example, if they run on the .NET 3.5 Framework. And in fact, there are some very interesting things happening at the moment in the world of Windows containers. There's a new technology currently in preview called Linux containers on Windows, which will actually allow a Windows Docker host to run both Windows and Linux containers side by side. And in this course, we'll be using both types of container, and I'll be showing you Azure services that are also able to support both types of container.

Docker Benefits
Now, you might be thinking that Docker seems quite similar to virtual machines in that it allows us to run applications along with their dependencies in an isolated way. So let's consider the benefits of Docker and compare them to the benefits that we get with virtual machines. Well, first of all, Docker containers offer great isolation. The code in your container can't interfere with other containers or applications running on the Docker host. And VMs also share this strength, offering strong isolation through hypervisor virtualization. Another benefit they both share is portability. If I package my application as a Docker image, then it will run the same on any Docker host, just like a virtual machine image can be moved to run on another host. But running containers is much more efficient than running each application in a separate virtual machine. Containers use a lot less disk space thanks to the efficient sharing of layers, and they also have lower memory requirements. And this means that you can pack often in the range of 10 to 50 times more containers on to a single host server than you could run virtual machines on that same server. Containers are also much faster to start than an application in a virtual machine. There's no need to wait for the operating system to boot up. In fact, there's minimal speed difference between launching a container running an application and starting that same application natively on your host operating system. Containers are disposable. I can easily delete them and leave no trace and replace them with a new container based on a newer version of the software if I want to upgrade my application, whereas VMs require patching of the operating system and the application that's installed on it, so there's a much greater maintenance overhead. And because containers typically only contain the bare minimum dependencies needed to run the application, they tend to be more secure with a much smaller attack surface area, unlike a virtual machine who's got a whole operating system installed that needs hardening. In fact, there are multiple security benefits to using Docker containers. The isolation means that one rogue container is not able to access data stored in other containers. And we've already mentioned that they have a very small attack surface area. But there are also services available that will scan container images for known vulnerabilities or viruses, as well as ways of checking that a container image has been signed so that you can be sure they've come from a trusted publisher.

Orchestrators
Of course, with all the benefits of containers that we've just looked at, it's not surprising that many people who've adopted containers quickly find that they're creating distributed applications that consist of multiple containers. And these applications are typically not just run on a single host server, but on a cluster of servers for reasons of scale and reliability with each server in the cluster responsible for running different containers. But this introduces some operational challenges. How can you manage a large number of containers? You need help deploying them, scaling to multiple instances, monitoring them for errors, configuring their network so that they can communicate with each other, upgrading them to new versions, maybe with a rolling upgrade, and moving them between servers to repair problems if there's a hardware failure or if one server is overloaded. And that's where container orchestrators come in. You still have a cluster of virtual machines, usually called nodes, each of which is able to run containers, but you also have an orchestrator that decides which of the nodes in your cluster should run the containers. And so you communicate with the orchestrator and describe what containers form your application, and often, for the purposes of resilience and scale, you might ask it to run multiple instances of some of your containers. The orchestrator then selects an appropriate node to run each of your containers, ensuring that they're spread out across the cluster. And there are several orchestrators available. One of the most popular, which we'll be looking at later in this course, is Kubernetes. And another one we'll be looking at is Azure Service Fabric. And we'll see that with orchestrators we can define our whole application in a declarative way, often in a format like YAML. And so the YAML file will define all the containers that our application consists of, and what versions they should be at, and how many instances of those containers we want, and what network topology we need, what data volumes we want to be mapped, and whether there are any specific constraints in terms of memory requirements or CPU usage. We may even have a mixture of Windows and Linux containers, and so they would need to be hosted on a node that supports that type of container. So orchestrators are able to handle a huge amount of the operational overhead involved in managing containerized, distributed microservice applications.

Containers in Azure
Finally, in this module, let's look at the options at our disposal for running containers in Azure. And there is a lot of choice available here. First of all, you could go the IaaS approach and just create a virtual machine with Docker installed and use that. So for running Linux containers, you'd use a Linux VM, and for Windows containers, you'd use Windows Server 2016 or 2019 with the containers featured enabled. And you could also spin up multiple virtual machines and install an orchestrator like Kubernetes and configure an entire cluster. And these are perfectly viable options if you're comfortable with configuring, and maintaining your own virtual machines. But that does bring a fair amount of operations overhead to it. And Azure offers several services that make hosting containers in the cloud really straightforward. And the four that we're going to focus on later on in this course are, first of all, Azure Container Instances. And this is like serverless meets containers. You just say what container image you want to run, and behind the scenes Azure will find a server to run your container. And this makes it just about the fastest and easiest way to get a container running in Azure. It uses a per-second billing model, so it can be extremely cost effective if you're just needing to run a container for a short period of time. However, if you're going to run your container 24/7, then you might want to consider one of the other hosting choices. There's also Azure Web Apps for Containers. Azure has a great web-hosting platform called App Service, and it contains lots of features specifically useful for hosting websites, such as setting up custom domains or configuring auto-scaling. And what Web Apps for Containers allows you to do is provide your web application as a container while still benefiting from all the added value that App Service offers, so it's a really great choice if what you're building is a web app. Then there's Azure Service Fabric. Azure Service Fabric is a powerful orchestration platform designed to host scalable, distributed apps. It's used to power many of Azure's own services, including things like Cosmos DB or the Bing search engine. And it supports several programming models, including providing your microservices as containers. More recently, Service Fabric Mesh has been announced, which greatly simplifies the effort involved in publishing an application to Service Fabric. And finally, there's Azure Kubernetes Service. AKS basically takes away the work of installing and configuring a Kubernetes cluster. Azure fully manages the control plane for you, and all you have to do is specify how many nodes you want in your cluster. And so this is a great option if you're already familiar with Kubernetes or you want to take advantage of its orchestration capabilities and use other open-source tools that work with Kubernetes. So, as you can see, there's a lot of choice, but please don't be overwhelmed. As we go through each of these four container hosting services later on in this course, I'll go into a bit more detail about why you might want to use it, and I'll show you how to get up and running with each one.

Module Summary
In this module, I gave a high-level overview of Docker. We learned what a Docker container and an image are, as well as the purpose of orchestrators, and we saw that there are several benefits of containers, including isolation, efficiency, consistency, portability, security, and much more. And these benefits explain why Docker has exploded in popularity over recent years and why so many people are choosing it as the basis for their distributed, cloud-based applications. Finally, we saw that Azure offers as many ways to run individual containers or whole containerized microservice applications. And these were Azure Container Instances, Azure Web Apps for Containers, Azure Service Fabric, and Azure Kubernetes Service. In the next module, I'm going to show you how to get started with Docker, and we'll see how to use it to run containers locally.

Running Containers Locally
Module Introduction
[Autogenerated] hi Markeith here and in this module, we're going to learn about how we can run containers locally. We'll start off by seeing how to get Dr Set up on your computer, and we'll learn about Dr Desktop for Windows on the two modes it operates in, then will actually run some containers locally, using the doctor Command line Till, and we'll learn about several useful doctor commands, including Dr Ron, to start a new container. Dr. P s to see what containers of running daca logs to examine the log output from a container. Dr Execs to run a specific command inside a container will also use Dr Images to manage images and Dr Volumes to manage volumes. And we'll see how to use Dr R. M to remove a container, So let's get started.

Installing Docker
[Autogenerated] Let's talk first about how we can install doctor. There are two main environments where you might want to install doctor in production or on your development machine. Now, if you're configuring a server to run containers in production, then for Lennox Containers you typically choose a version of Lennox that's quite stripped down and has the minimal support needed to run your containers. And some popular choices here are Alpine lennox or you Bunty Corps or Container lennox. And if you're wanting to run Windows containers in production venues, install Windows Server 2016 or 2019 on, make sure that the containers feature is enabled. And, of course, if you're using one of the azure hosting options that we're going to be learning about later on in this course, then you won't need to install doctor at all. It comes ready configured for years, but in this module we're going to focus on running containers and locally on our development machine. So how do you go about installing doctor on? You're like a machine. Well, it depends on what operating system you're running on. A great starting point is to visit this page on the Docker documentation site at doc. Stop dr dot com forward slash install on this page provides detailed and up to date instructions for how you can install Dr C, which stands for Community Edition and it's free. Dr. Do will set off for a commercial option called Enterprise Edition, which adds in additional features like security scanning of your containers under Lennox Weaken, see instructions for how to install doctor on a number of popular distributions such as you, Bunty and I won't walk you through all the steps. But basically, after running through a few preliminary steps to configure the apt repositories, you can install Doctor on Lennox with this pseudo apte. Get in store command. Don't. If you're on a Mac, well, it's a case of downloading. It's from the doc, a store running the installer and dragging Moby the Whale into the Applications folder. What about windows? Well, if you're running on Windows 10 Pro Addition, you can install Docker desktop for Windows, And the reason you need Pro Addition is that Dr Desktop for Windows requires the hyper V Windows feature, which isn't available in the home edition of Windows 10. If you're still using Windows seven or you don't have the prohibition of Windows 10. Then you can use something called Dr Toolbox instead, which uses Oracle virtual box instead of hyper V. However, if you are able to install Doctor desktop for Windows, then I do recommend it. And for Windows, all you have to do is download and run the installer. And so if you want to follow along with the examples in this module and you haven't got doctor for Windows or Doctor for Mac installed, then why not do that now? By the way, when you install doctor for Windows, it might not be configured to start automatically, but you can always launch it manually by searching for DR for Windows in your start menu. And when it started up, you'll see a doctor icon in your system tray, which brings up a menu when you right click it. If you select the settings option, it brings up this settings dialogue where you can set it. Toe Auto starts when you lock into windows, as well as configure various other useful options. Now, one very important configuration option two notes with Doctor for Windows is that it has two modes. You can switch between Windows and Linux. In Windows mode, you can run windows containers on in Lenox mode. You can run lennox containers, and you can switch between the two modes at any time by right clicking on the docker icon in your system tray and choosing the menu option to switch mode. Now you can check what mood you're in by using the doctor version command from a command line and so he can see. If I run the darker version command and I'm in Lenox mode it was Show me the version I've got installed on that. The server OS is Lennox, and I can switch him out by using that right click context menu, and it will take a few moments to complete. But once it's done, I can run darker version again, and now you're noticed that the server is reported as Windows

Demo: Docker Run
The Docker command-line tooling offers a wide variety of commands, but the most important one that lets us run our containers is docker run. Let's start off by looking at a typical docker run command. And let me explain what's going on here. We're asking Docker to run a new container using and image with the name itzg/minecraft-server. And if we don't have that image locally, which we won't if we've just installed Docker, then it will go off to Docker Hub, which is the default container registry, and try to pull down an image with that name. And here you can see this image on Docker Hub, and it gives us some basic information about the image and how to use it. This is an image that somebody with the username itzg has created, and it's just got the Minecraft server installed. Now, if I look on the Tags page, you can see that a Docker image can have multiple different tags. And in fact, this collection of Docker images is referred to as a repository. There are many images that all share the same name, but they have different tags. And so here we're looking at the Minecraft server repository, which has several images, each identified by a unique tag. Now, on the docker run command I showed you, I didn't initially specify a tag, and so that just means it will get the latest. But if we want to, we can ask for a specifically tagged image, and in fact, that's a good practice to do so you can be sure which one you're getting. And we do that with a colon after the image name, and then we specify the tag like this. So here, we're asking for the image that's tagged raspberrypi. Now, the -d flag is quite commonly used, but it's just short for detached. What that means is we're going to run this container in the background, so although the container will start up, we won't see its console output in our command prompt window. And the -p flag allows us to publish a port. And the syntax for this is the host port followed by the container port. So we're saying that port 25565 on the container should be exposed as the same port number on the host. But of course, we could change the host port if we wanted to have multiple Minecraft servers all running at the same time. Finally, I'm also showing that we can give our container a name. This is optional. If you don't name your container, it will just get a randomly generated name, and it will also have an ID that you can use to refer to it in other Docker commands. But let's do a demo now. Suppose we wanted to try out Redis, but we'd rather not install it directly on our local development machine. Let's see how easy Docker makes it to run up an instance of Redis and experiment with it. So here I am at my command prompt on a machine running Docker for Windows, and I'm currently in the Linux mode. And I'm going to start off by issuing a command very similar to the one we just looked at. It's another docker run command. And I'm asking to start a new container based on the Redis container image. I'm running it in the background, so with a -d flag, and I'm just going to give this container the name redis1. And I'm going to expose the container's port 6379, which is the Redis default, and I'm going to expose that on port 6379 on my local machine. And that means that I could actually connect to this Docker container locally if I needed to. Now, when we run this for the first time, it will need to download the Redis image from the Docker Hub container registry to our machine. And this is a one-time action though. Once a container image is available locally, starting up new containers based on the image will be much quicker. And once the Docker container has started up, it will print out the ID of the newly created container. And I can use the docker ps command to see what containers are running. And so when I issue this commend, I can see the container ID, the image name, the startup command that the container is running, when it was created, whether it's running or not, what ports we've published, and what the container name is. Another very useful command we can run is docker logs. If say docker logs redis1, that will show me the log outputs from the container named redis1. If I hadn't given this container a name, then I could just use the first few characters of the container ID instead, which is a convenient shortcut. And here we can see that our Redis container has started up successfully, and it's ready to accept connections. We can also see the container image we downloaded with the docker image ls command. This shows me all the images I have locally. And so periodically I might want to do some cleanup to delete any that I no longer need. Here we can see that I have the Redis image and that it's about 83 MB in size.

Demo: Docker Exec
In our last demo, we started a Docker container running Redis, and we exposed port 6379 on it. And that means we could us the redis-cli and connect to our container on local host port 6379. But let's suppose that we don't have the redis-cli installed locally. I actually don't on this computer. Can we use Docker to run that as well? And the answer is yes, we can. We can use cocker exec to run a command inside a container, and our redis1 container actually already has the redis-cli installed. So let's start off by seeing how we can run another command on the container that we're already running. What we're saying here is we want to execute the sh command. So we're just opening a shell, and we're going to do this against the redis1 container. The -it flag means that we want to run this in interactive mode, where we essentially attach our console to this process in the container, and so we can see its output and type commands in. And so the -it flags are often used in the Docker run command instead of -d when we don't want to be detached and run in the background. Okay, let's execute this now, and as you can see, we're now inside a shell in our container. And so if I do something like ls -al, I can see the contents of the file system in the container. So let's run the redis-cli with redis-cli, and this will just attach me to the locally running Redis inside this container. And I can check it's running by typing ping, and I get PONG back, which is a good sign. Let's just run a few basic Redis commands. First, I'll set a key in the Redis cache with set name mark, and I can retrieve that key with get name. I can also increment a new counter with incr counter. And let's do that twice. And now if I say get counter, it will show us that the counter has a value of 2. Okay, so let's exit from the redis-cli with exit, and now we're back in our shell on the container, and I'll exit from that as well with exit. And so now we're back at our Windows command prompt. So let's see if we can take this idea a step further. Can we run the redis-cli in a different container and connect back to this first container? Well, yes, we can actually do that, and we can do that with the docker run command, and this will introduce us to a few new flags. So let me show you first the command that we're going to run, and then I'll explain it to you. Now, some of this is already familiar to us. We're going to run the Redis image again, so we're going to have two containers running from the same image, and we're going to name this container client1. And instead of the -d flag, we're going to use the -it flag just like we did with docker exec so that we get an interactive session. We're also specifying that we want to run a shell in our container. That's what this final argument does. And I've specified the -rm flag, which says that when the shell session we're running in this container ends, we want to just remove the container. And this is a handy way of avoiding needing to do some manual cleanup afterwards. Finally, I'm linking this container to our redis1 container with the --link argument. And this gives containers a way to communicate securely with each other. And I'm saying that within this container I want to be able to access the redis1 container, and I want to refer to it by the name redis. Okay, let's run this, and as you can see, because of the -it argument, we're straight into the shell. So now I'm going to run the redis-cli again, but this time with the -h argument to specify the hostname we want to connect to. Remember in the link command I said that I'm going to call the redis1 container redis inside this container. So I'll say redis-cli -h redis, and this will attach the CLI to the other container, the redis1 container. And just to prove that we really have connected to that other container, let's ask for those values that we set in the cache. So if I say get name, I see mark again, and if I say get counter, we can see it's at 2. So we really are connected to that other container. Let's exit from the redis-cli now and from the shell. And because we specified that -rm flag in our docker run command, this container has already been deleted. So if we type docker ps, we can see that the client1 container is not there, only the redis1 container. And if now we want to stop the redis1 container, we can do so with the docker stop command like this, just saying docker stop and the name of the container. Now, when a container is stopped, if we run the docker ps command, it doesn't show up, but we can say docker ps -a, and that will show that our redis1 container is still there, but it's in a stopped state. Now we could restart it if we wanted to, although since redis is an in-memory cache, it will have forgotten those keys that we set. But let's clean up after ourselves. I'm going to delete this container now with docker rm redis1, and that removes this container. And so the only thing left now is the Redis container image. And that remains on our disk by default, which means if we wanted to start up any new Redis containers in the future, they're going to start up very quickly. But let's suppose that I decide I don't need to use Redis anymore, and I want to free up the local disk space that that image is taking up. What I can do is I can see what images are on my machine with the docker image ls command. And if I want to delete the image, I can say docker image rm redis. And you might remember that we mentioned in our last module that Docker images are made up of layers, and so you can see that all of the layers that make up the Redis image are being deleted here. And so that's everything we created in this demo cleaned up. We've been able to use Docker to experiment with Redis, and we've left nothing behind when we're finished.

Demo: Docker Volumes
Docker containers are designed to run your applications, but they're not intended for storing data long term. If you want to upgrade an application, you don't upgrade the software that's inside the container. Instead, you dispose of the container and just create a new one with an updated version. But what if you have data associated with your container that you want to keep? Well, that's where Docker volumes come in. We can mount a volume to a container, which gives it a place to write data that has an independent lifetime of the container itself. And that means that if we delete the container, we can just reattach the same volume to a new container. So let's see this in action with a Postgres database. Here's the command that we're going to run to create a Docker container running Postgres. And we've seen some of these flags before. We're running in detached mode, so in the background, and we're publishing the default Postgres port of 5432, which means that we can connect to this database from our local machine if we want to. And we're naming the container postgres1, and we're using a Postgres image, which, again, will be downloaded from Docker Hub. And to set up a volume, we're using the -v argument. The first part of this is the volume name, postgres-data, and the second is the location inside the container that this volume will be mapped to. In this case, var/lib/postgresql/data, which is the default location that Postgres will store any databases we create in. And so as far as Postgres is concerned, it's just writing to that location, but actually, anything that's written there will end up in our postgres-data volume. Now, we haven't yet got a volume called postgres-data, but Docker will create one for us to use the first time we run this command. So, if I run this, it's first of all going to download the Postgres image if I don't already have it locally, and it's going to start my container running in the background. So we'll just wait a moment for this to complete. And now, let's use the same technique that we used before to execute some commands within the container. I'm going to do a docker exec on my postgres1 container and open another shell inside it. And when I'm inside that shell, I can create a new database with the createdb command specifying the Postgres user and a database name of mydb. And then I'm going to use psql, which is the postgres-cli to connect to that database with the following command, again, specifying the Postgres user and what database I want to talk to. And of course, again, these tools are not installed on my local Windows machine. I'm running them inside the container. And so now I'm connected to that mydb database. Let's create a table called people with a CREATE TABLE command, and let's just insert a new row into that table. And so now I'm going to exit the psql CLI with \q, and I'm also going to exit from this interactive shell session with exit. Okay, so in our container, we've created a new database, and we've put some data in, but I'm actually now going to delete the container. Now, last time we deleted a container, we did docker stop and then docker rm, but we can do both of those in one step if we say docker rm -f to force it to stop and the name of the container we want to delete. So now we've deleted our postgres1 container, but our database isn't lost because that was storing its data in to a volume. And so if I run the docker volume ls command, which will show me all the volumes I have, we can see that the postgres-data volume is still present even though the container that was using it has been deleted. And so what that means is I can now start a brand-new container from the Postgres image and mount that same volume. And so let's call this new container postgres2. And I start it up with very much the same command that we saw before. I'm mounting the same volume to the same location, and the only difference is I'm giving this a new name. And this time it's super quick to start up because we've already got the Postgres image locally on our machine. And so now that's running, let's use the docker exec command again to run an interactive shell on this container, which will use exactly the same docker exec command that we saw before, only we're talking to the postgres2 container. Once we're in, we'll run the psql command-line tool again and connect to the mydb database. And we can do that because the volume has already got that database in it. And so once we're connected to that database, I can just run a basic SQL query and see that the data is still there. So we've manage to persist our database that we created in one container, deleted the container, and then reattached it to a completely different container. And so let's exit psql now with \q and exit from our shell with exit. And now I'll delete this container with docker rm -f for force and postgres2. And if I decided that I don't want my database anymore, I can delete that with docker volume rm and then the name of the volume, which is postgres-data. And this deletes the volume which contains the database we created.

Module Summary
In this module, we've learned about how you can run containers locally with Docker. The main Docker command that we learned about was docker run, which lets us start a new container from the specified image. And we learned about several options for docker run including -d for running detached, which means in the background, or -it if we want an interactive terminal. We saw -p for publishing a port from the container, -v for mounting a volume, --name to give our container a name, --link to allow our container to talk to another container, and --rm to delete our container automatically when it stops running. And of course, there are plenty of other useful options that we've not seen yet, such as -e, which allows you to set environment variables in the form NAME=value. And some of the Docker commands that we used in this module are docker exec to run a command against an existing container, docker ps to see what containers are running, and docker logs to view the output from the container. And I also showed you the commands you need to clean up after yourself. So we can delete containers with docker rm -f container-name, we can clean up images with docker image rm image-name, and we can clean up volumes with docker volume rm and the volume-name. So I hope this module has given you a feeling for what sort of things can be accomplished with Docker locally. We've only scratched the surface of its capabilities, so if you are brand new to Docker and you'd like to go a bit deeper, then do check out a few more courses here on Pluralsight, such as Getting Started with Docker by Nigel Poulton or Getting Started with Docker on Windows by Wes Higbee. And in the next module, we're going to learn how to create our own Docker images and publish them to an online container registry.

Creating Docker Images
Module Introduction
Hi, Mark Heath here, and in this module, we'll learn how we can create our own custom Docker images and upload them to a cloud-hosted container registry. We'll start off by learning what a Dockerfile is and some of the key instructions available to us in Dockerfiles. Then we'll see how we can build an image from a Dockerfile with the docker build command, and then we'll see how to tag those images using the docker tag command and how to push them to a cloud-hosted container registry with the docker push command. And along the way, I'll introduce you to the really useful Azure CLI command-line utility, which we'll be using to create an Azure Container Registry.

Creating Images from Dockerfiles
In our last module, we created some containers based on existing Docker images hosted on Docker Hub, and these were the redis and postgres images. But what if we wanted to create our own Docker images? That's where a Dockerfile comes in. A Dockerfile contains instructions for how to create a Docker image. So let's look at a very basic Dockerfile for an ASP.NET Core application first. The first instruction in a Dockerfile is FROM. This specifies the base image that we're running on top of. So here we're saying that we want to use the microsoft/dotnet image with the aspnetcore-runtime tag as our starting point. And this image has everything that's needed to run ASP.NET Core applications. Next, we're using the WORKDIR command, which sets the working directory for other instructions like RUN, COPY, or ENTRYPOINT. Our working directory is the app folder, and if this doesn't exist, it will get created. Next, we can use the COPY instruction to copy files from our local machine into the image. And so here I'm assuming that we've already built and published our .NET Core application into a folder called out on our local development machine, and so we'll copy the contents of that folder into our container's working directory. Finally, the ENTRYPOINT instruction tells the container what to run when it starts up. In this case, it's going to run the dotnet application with samplewebapp.dll as the parameter. So how do we build a Docker image from this Dockerfile? Well, we can just use the docker build command if we're in the same folder as the Dockerfile, and the -t argument is specifying that we're giving our image a name, so I'm calling it samplewebapp here. So let's see a demo of this in action. I'm currently in a folder that contains that Dockerfile that we just looked at, and this folder also contains a very simple ASP.NET Core test application. And so first I'm going to build that ASP.NET Core application with the dotnet publish command specifying I want a Release build and that the published binaries should be put into a folder called out, which is what our Dockerfile is expecting. Now, by the way, don't worry if you don't have the .NET SDK installed because later on in this module I'm going to show you how we can build this Docker image without needing any other dependencies installed. But now we've built our dotnet application, let's build our Docker image with docker build -t samplewebapp. So that's the name and optional tag which I haven't used here that I'm giving to this Docker image, and the dot at the end means we're just running in the local folder, so it's going to look for a Dockerfile in this folder. Now, when we run this command, it will take a little while on the first run because we're going to need to pull down the base image, which was the microsoft/dotnet image, if you remember. But once that image is finished downloading, the rest of the steps will complete quite quickly. So now we've created our image. If we do a docker image ls, we can see both the dotnet image and our samplewebapp image. Now, the size of samplewebapp might look large, but remember, because of the layering of Docker images, it means that most of this space is simply the base image, so it's actually only taking up an additional megabyte or so of space in addition to my base image. Let's run a container from this image with docker run in detached mode, and I'll expose port 80 on the container as port 8080 locally. I'll give my container a name of myapp, and we're running the samplewebapp image that we've just built. And so once this is running, because we exposed port 8080 locally, I can visit localhost port 8080 in a web browser, and sure enough, we can see that our ASP.NET Core application is running. Now this isn't a very exciting app. It just shows some diagnostic information, like displaying various environment variables, which we'll be using later on in this course. But this is sufficient for now to demonstrate the fact that we can easily run a .NET Core application inside a Linux container using Docker for Windows. So let me stop and delete my container now with docker rm -f to force it because it's still running and the name of the container, which is myapp. Remember that this only deletes my container. The samplewebapp image is still present for us to create other containers from it if we want to.

Creating Dockerfiles
Now, you might be thinking that it looks a little bit tricky to create Dockerfiles, and the good news is that often you won't need to. First of all, there are almost always great example Dockerfiles that you can copy for whatever framework or programming language that you happen to be using. For example, the ASP.NET Core documentation has a whole page devoted to explaining how you can set up a Dockerfile for an ASP.NET Core application, and it provides links to sample Dockerfiles on GitHub that you can simply copy and adjust for your needs. And so here's that page on the ASP.NET Core documentation site that you can refer to if that's what you're building. But on top of just copying example Dockerfiles, often you'll find that the tooling for your language supports auto-generating Dockerfiles. For example, Visual Studio 2017 has the option for you to right-click a project and say Add Docker support, and that would generate a Dockerfile for you appropriate to the project type that you're building. You might also be familiar with the open-source Yeoman project, which is able to scaffold new projects for a whole variety of different languages and frameworks based on plug-in generators. And Microsoft have provided a Docker generator, which means you can simply type yo docker to get a Dockerfile created for you. And you can find the Docker Yeoman generator on GitHub at the Microsoft/generator-docker project. And you may also find that the CLI tools in the SDKs that you're using can generate Dockerfiles. For example, if you have the Azure Functions CLI installed, then you can get it to create a new Azure Functions project complete with a Dockerfile by using the func init --docker command. And so it's not very often that you'll need to write your own Dockerfile completely from scratch. There are plenty of options that will help you to get started quickly.

Multi-stage Dockerfiles
The Dockerfile that we looked at in our last demo assumed that we'd already built our .NET application locally and we were just copying the output from the .NET publish command into our container. But one really nice possibility that Docker opens up to us is to use a Docker container not only for running our application, but for building it as well. And this means someone can just clone the source code and build it with docker build without needing to have any developer frameworks installed locally. So in our example, that means we wouldn't need to have the .NET Core SDK installed. And this approach means that you can easily build applications whether they're using .NET Core, or Ruby, or Java, or Python. All you need installed is Docker. And this technique uses what's called a multi-stage Dockerfile. The first stage copies our source code into a container that has the necessary SDKs to build the application. And this could be quite a large image, but that doesn't matter because the second stage copies the built application out of that container and into another more streamlined container that only has the necessary runtime needed to run the application. So let's see how we can do that. This Dockerfile has more steps than the previous one that we looked at, but it's mostly using the same commands. Let's talk through what it's doing. First of all, the FROM instruction at the top now specifies the microsoft/dotnet image tagged sdk, and this version of the image includes the full .NET Core SDK. So it's a much larger image, but it has everything that's needed to build .NET Core applications. And notice that there's also an AS statement, which allows us to name this stage of our container building for use later, and we're naming it build-env. We're going to set the working directory to app as before, but then we just copy in the csproj file from our source code and run dotnet restore. The RUN instruction allows us to run any command within our container as part of the image-building process. Then we copy in all the rest of the source code and use the RUN instruction again to perform a dotnet publish in Release mode, sending the output to the out directory. Now, if you're familiar with how to build .NET Core applications, you might be wondering why we didn't just copy everything in in one go and then run dotnet publish. And the answer is that each instruction in this Dockerfile creates a new layer, and whenever we do subsequent builds of our Dockerfile, we only need to recreate the layers that have changed. And so the dotnet restore step, which restores NuGet packages, is making its own layer, and we'll only need to change that layer if we change the NuGet packages that our project depends on. However, the code is much more likely to have changed every time we build this image, so we're doing this for efficiency. Now, once we've built our .NET Core application, we don't just put the ENTRYPOINT instruction after dotnet publish, even though that would actually work because the dotnet sdk image is able to run our application. Instead, we want the container image we build to be as small as possible, so we have another FROM instruction to define a second container, which is going to form the basis of our output image. The first container, which we called build-env, only had the job of building the application, and the second container uses the dotnet image tagged aspnetcore-runtime, which we used in our previous Dockerfile, and, as we said then, is a highly optimized base image that contains only what is necessary to run ASP.NET Core applications. And so now, inside this second stage, we set the working directory and then we copy the built application with the COPY instruction. Only this time, we're not copying from our local disk; we're copying out of the build-env container, which is what this from argument is doing. Finally, the ENTRYPOINT instruction is the same as before, telling our container to run dotnet samplewebapp.dll when it starts up. So now I've explained how a multi-stage Dockerfile works, let's see it in action in our next demo.

Demo: Building an Image from a Multi-stage Dockerfile
In this demo, we're going to build another Docker image, but this time using the multi-stage Dockerfile that we just looked at. And this time, our docker build command is going to look slightly different. First of all, I'm naming the image samplewebapp again, but I'm tagging it v2. And secondly, that Dockerfile that we were just looking at, I've named multi- stage.Dockerfile, which isn't the default, so I'm using the -f argument to specify which Dockerfile should be used by docker build. And the dot at the end of this command means we're just working in the current local directory. So when we run this, there's another image that needs to download because I haven't already downloaded the sdk image, and this one is a bit larger, but when that finishes, and I'll fast forward the video a little bit to save you waiting, we can see that it's actually performing the build with a dotnet restore and then a dotnet publish before it finally copies that built application into our runtime container. Okay, so once this is all done and we've created our new container image, we can run it with the docker run command. The name of the image that we're running is samplewebapp, tagged v2, I'm going to name this container myappv2, and let me also show you us setting an environment variable. I'm going to set the environment variable whose name is TestSetting to Multi-stage, and that's because my test application is going to show us the value of that environment variable when we load the web page. So once this container has started up, we can test that it's worked by visiting localhost port 8080 just like we did before. And as you can see, the application is running inside the container, and we can see that that environment variable that we specified in the docker run command is available too. So the great thing about having this multi-stage Dockerfile is that anyone who clones our source code and has Docker installed can build and run the container image even if they don't have the necessary SDKs, like the .NET Core SDK, installed locally. In our next demo, we're going to see how we can push this image to a container registry, but first, let's talk about what a container registry is.

Container Registries
Now, so far, the Docker images that we've created have all been stored locally on our development machine, but what if we wanted to make those available to other developers or to a Docker host that's running in Azure? What we need is a container registry. And we've actually already been using a container registry. Docker Hub is a container registry that hosts thousands of publicly available Docker images, such as redis and postgres that we used in the last module and dotnet that we used in this module. And you can create a free account on Docker Hub and push your own container images to that account. And this is great if you're doing open-source work and you want to share your images with the general public. But often with containers you're building private images that you intend only to share with other developers on your team, and you want to be able to run them in the cloud as part of your application deployments. In that case, what you need is a private registry. And you can pay to enable private container image hosting on your Docker Hub registry, but another option that you should consider if you're using Azure to host your containers is the Azure Container Registry. This allows you to run your own private container registry running in the same Azure data center as you plan to host your applications in, and this has the benefit of reducing network latency and minimizing data ingress and egress costs. And you can also replicate your registry across multiple Azure regions if you want. Azure Container Registry also supports a really nice feature that lets you build your container images in the cloud, and this allows you to automate the building of new images not only when you're checking new code, but also when the base images that your containers are built on are patched. So let's see in the next demo how we can create an Azure Container Registry.

Demo: Creating an Azure Container Registry
In this demo, we're going to create an Azure Container Registry. And there's a few ways you can do this. You could do it in the Azure portal or with PowerShell, but the tool that I'm going to use is the Azure CLI. And this is a really simple-to-use, free, cross-platform, command-line utility that supports most resource management tasks in Azure. You can install it locally on your development machine, and you can even run it directly in the cloud through an Azure shell accessible in the Azure portal. And if you'd like to learn more about the Azure CLI, including how to install it and to log into your subscription, then do check out my Azure CLI Getting Started course here on Pluralsight where I explain all of the getting started steps and plenty of examples of what you can do with it. But I've already got it installed on this machine, and so here I am at a command prompt, and I'm going to use the Azure CLI to create a new container registry. Now if you're using Azure CLI for the first time, you will need to log into your Azure account with the az login command. I've already done that. And if you've got more than one Azure subscription, then do also make sure that you've called the az account set command specifying the subscription name with the -s argument just to make sure you're using the right subscription. Okay, so we're going to start off by creating a resource group. I'm going to give it the name PluralsightAcr and store that in a variable to reuse in future commands. And I'm going to create the resource group with the az group create command specifying the name I'm giving to the resource group and the location that I want to put it in, and I've chosen a location close to where I am in West Europe. Next, let's pick a name for our registry, which does need to be unique across Azure, so I'm going to call this Pluralsightacr, and we'll create the Azure Container Registry itself with the az acr create command. I specify the resource group I'm putting it in, the name of the registry. I need to select a SKU, which can be Basic, which is the cheapest option, and it just supports a small amount of storage. There's also Standard, which offers increased storage limits and image throughput, and there's also a Premium tier, which has even higher performance and supports geo-replication. And you can check out the differences in pricing in detail on the Azure website to choose which pricing tier best fits your requirements, but for this demo, Basic is going to be more than sufficient. Finally, I've also set the admin-enabled flag to true, which is going to allow us to get hold of a username and password to log into this registry. We don't actually need that now, but it is going to be useful for some future demos, so I've turned it on here. And once this command completes, now we've got an Azure Container Registry that we can push our local images to. And one of the great benefits of having a private Azure Container Registry is security. It means that we don't have to share our images with the general public. It means we can restrict who has access rights to push images to our registry. And we can also use it to ensure that only images we trust are used in our cloud deployments. And we can also use features like ACR Build to automatically produce new versions of our container images whenever their base images are patched with security updates. So if you are planning to deploy containers to Azure, I do recommend making use of Azure Container Registries. Next, let's see how we can push our images to the Azure Container Registry that we just created.

Demo: Pushing an Image to Azure Container Registry
In this demo, we're going to push a local image to our Azure Container Registry. And there's a couple of things we need to do to get set up. First, we need to log into the Azure Container Registry, and we can do that with the az acr login command specifying the name of our container registry, which, if you remember, we stored in the registryName variable. Now we also need to tag our images in a special way before we push them, and that requires us to know the fully qualified name of our ACR login server. And there's a convenient Azure CLI command that we can use to get that, which is az acr show, and I'm going to ask it to show information about our registry and perform a query to just get the loginServer name. And once I've run this, we can see that for our Azure Container Registry, the loginServer is Pluralsightacr.azurecr .io. And so now that we've got this information, we can tag our image, ready to push it. Remember that I already had an image called samplewebapp with the v2 tag. Let's give this image a new tag, which we can do with the docker tag command specifying what existing image I want to give a new tag to and then specifying the new name and tag that we want to give it. The new name has to start with the ACR login server that we just retrieved, and then a slash, and then I can add whatever name and tag I like after that. So I'm just using samplewebapp and v2 again. So now we're ready to push our image, and we can use the docker push command and the tag that we just gave the image. And Docker has the credentials that it needs to communicate with that Azure Container Registry because we ran the az acr login command. And this might take a minute or two to complete, but once it's pushed the image up, it's available now in our private Azure Container Registry. And we can use the Azure CLI to see a list of all the images that are in our container registry with the az acr repository list command specifying the name of our container registry. And we can see that we have that one image in there. And you may remember earlier in this course that we said when you have multiple images that have the same name but different tags, that group of images is called a repository. And so we can see all the tags in this repository with the az acr repository show tags command to see all the tags for our samplewebapp repository that's here in the registry. And again, currently there's just one tag, the v2 tag. But we could push multiple versions of this same application, and they would all be able to coexist on the container registry. Now, obviously, you might want to delete old images that are no longer in use, especially if you're using one of the cheaper pricing tiers with limited space. And so if we want to delete this container image from the Azure Container Registry, we can do so with the az acr repository delete command specifying the container registry name and the repository name and tag that we want to delete. So here I'm deleting samplewebapp tagged v2. And it will prompt us for confirmation, and if I say yes, our image will be deleted. Now, by the way, if we wanted to delete the whole ACR we created, we could do that with az group delete, passing in the name of the resource group, which would delete the whole resource group and everything in it, which, at the moment, is just a single Azure Container Registry. And so here you can see the command to delete this resource group. But I'm not going to do that at the moment because this container registry will be useful in future modules where we're actually going to start running containers in Azure.

Module Summary
In this module, we learned how to create our own Docker images. We looked at the syntax of a Dockerfile and saw some of the basic instructions including FROM, WORKINGDIR, COPY, RUN, and ENTRYPOINT. And we also saw that you can perform multi-stage builds so that the first stage compiles your code and then the second stage copies the compiled code into a more streamlined base image optimized for running your application. And then we learned how to build a Docker image from a Dockerfile with the docker build command. Then we looked at the concept of private container registries, giving you a secure place to host your Docker images in the cloud and enable them to be shared with other team members and accessed by Docker hosts running in Azure. And we saw that the Azure Container Registry is a great choice if you're planning to host containers in Azure, and we learned how to use the cross-platform Azure CLI to create an Azure Container Registry using the az acr create command. We logged into it with az acr login. We then tagged our Docker image with docker tag in a special way that allowed us to push it to the Azure Container Registry with docker push. Okay, so far in this course, we've learned a bit about how Docker works and how to create our own container images and publish them to a container registry in the cloud. The next step is to start exploring the various ways of running containers in Azure. And we're going to look at four different ways, starting in the next module with Azure Container Instances.

Running Containers on Azure Container Instances
Introducing Azure Container Instances
Hi, Mark Heath here, and in this module, we'll learn how we can run containers with Azure Container Instances. And this module is the first of four modules in which we're going to look at various ways to run containers in Azure, and each of these techniques has got unique strengths that will make them suitable in different scenarios. And so in this module, we're going to start off by seeing what Azure Container Instances can do and why you might choose them. And of course, then we'll see it in action. So, first of all, what is Azure Container Instances? Azure Container Instances is the quickest and easiest way to get a container running in Azure. One of its key strengths is that it's a serverless platform, and that means you don't need to provision any container hosting infrastructure to Azure before you run your containers. You simply ask for an Azure Container Instance to run, and behind the scenes the platform will ensure that there's a suitable server to run it on. And this makes Azure Container Instances ideal for quick experiments. Say you want to get Elasticsearch or MongoDB running in the cloud, just to try something out. Well, with Azure Container Instances, you can have it up and running in no time. And one of the best things about this service is that you only pay while your container is running. Azure Container Instances has a per-second billing model, so if I only need a container running for 10 minutes, I can create it, let it run for 10 minutes, and then stop it, and that's all I'll pay for. I don't have to worry about shutting down and deleting the virtual machine that was running that container. And the pricing model, which you can explore in detail on the Azure website here is simply based on how many seconds your container is running for and how much memory and CPU it's making use of. In some regions, there's an additional charge if your container is a Windows container, as you can see here in the East US, but in other regions, like Central US, there's no extra fee for Windows, but the duration costs are slightly higher. And there's a handy calculator that you can use, which we can access here. And so, for example, if I decide I want to run a Linux container in East US for 8 hours, which is 28, 800 seconds, and I want 2 GB of memory and 2 virtual CPUs, that will actually work out costing me less than a dollar.

ACI Use Cases
[Autogenerated] So what kind of use cases are as your container instances a good fitful? Well, many types of containerized workloads require permanently running containers. For example, a website or a database in a container needs to be continuously running, as it must always be ready to handle an incoming request. And so these scenarios are typically not the best fit for as your container instances, because the per second pricing model works out at about 10 to 15% more expensive than the equivalent virtual machine. If you're just going to leave your containers running 24 7 however, there are many scenarios in which you might want to quickly spin up a container for a short period of time and then stop it. One example would be a continuous integration server that's performing builds of your software whenever you check in. And that might actually spend a lot of time idol overnight and at the weekends. And so instead, if you spun up on a C I container whenever you built, that could potentially save a lot of money, and another advantage of spinning up a C I containers on demand like this is that you could spin at multiple instances of the same container. So if you suddenly needed to run several different see, I builds all at once. You could start up multiple instances, and they could do the builds in parallel rather than you having to wait for them to complete sequentially. I see I containers are also great for running short lived experiments. If you want to just get an instance of Mongo D, be running quickly just to try something out that's really easy to do. Or if you want to perform a low test, then you could spend up multiple containers that generate load and then shut them down when you're done with it as your container. Instances are also a great fit for batch jobs. Maybe that run for a few hours every night and one final great example of a good use case for Asher Container. This is is if you've already got a kubernetes cluster, and so you're paying for a cluster of virtual machines that are nodes in that cluster. But you have a highly variable workload. Maybe it's certain times the incoming traffic to a website is extremely high, or you have to ingest large amounts of data overnight, although it's possible to scare your kubernetes cluster up by adding extra notes as your container instances can be used to elastic lee handle bursts of load without you needing to provision extra hardware. And there's a special connector called the A C I connector for kubernetes, also known as the virtual Cube lit, which allows you to do just that, allowing you to treat as your container instances as a virtual note in your cluster, but essentially has infinite scale. So although as your container instances is not the right fit for every type of containerized workload, whenever you need to run short lift containers, toe handle, occasional workloads or high bursts of additional work, they can be an excellent choice.

ACI Features
Azure Container Instances are easy to create and manage using tools like the Azure CLI, PowerShell, the C# SDK, or with ARM templates. So there's plenty of ways you can automate the management of them, and in most cases that you're using them, you probably do want to automate their creation and teardown so you can take advantage of that per-second billing model and only pay when you really do need them. When you create an Azure Container Instance, you can configure various networking features such as giving it a public IP address and a prefix for its domain name, and you also get to choose what ports you want to expose. Azure Container Instances can run either Windows or Linux containers. Linux containers are currently faster to start up simply because their image sizes tend to be smaller, so it takes less time for the image to get downloaded onto the target server. And Windows containers currently also do have a few limitations and don't support all the features that Linux containers do, although hopefully that will change in the near future. Azure Container Instances support a restart policy allowing you to decide what should happen when the container stops, whether it should be automatically restarted or not. Azure Container Instances allow you to mount volumes. You can mount an Azure file share as a volume, which would likely be the most common use case, but there are other options available, such as mounting volumes to contain secrets or mounting Git repositories as a volume. When you create a container instance, you can optionally specify the command line that should be run, just like we've seen earlier in this course with the docker run command, which also allows you to override the default initial command for a container. And you can also specify environment variables and access the logs from your container. Now, a very important concept with Azure Container Instances is container groups. Every time you use Azure Container Instances, you're actually creating a container group even if you're only using that to host a single container. But a container group can consist of multiple containers, which will run together on the same server and share resources, and it's very similar to the Kubernetes concept of a pod, which allows you implement a pattern known as sidecar containers. Now, don't worry if that sounds a bit confusing. It will just help to make sense of the terminology when we get around to creating new container instances. We will actually always be creating what's called a container group even though we're only putting a single container in it.

Creating Container Groups with the Azure CLI
Although there are several ways you can create ACI container groups, my favorite way is with the Azure CLI, which I've already introduced earlier on in this course. The command to create container groups is the az container create command. And there are lots of options for this command. I'm just going to show you a few of the most commonly used ones here. So, we can specify the name that we're giving to the container group with -n and the resource group we want to put it in with -g. --image allows us to specify the Docker image name, and the easiest option here is to specify an image name from Docker Hub, but we'll see in a demo shortly that we can also provide credentials to authenticate with a private container registry to access an image in Azure Container Registry, for example. If we say --ip-address public, that means we want a public IP address for our container, which we'd want, for example, if we were hosting a website. --dns-name-label allows us to specify the prefix for a friendly DNS name. --ports allows us to specify which ports to open. --os-type should be used if you're running a Windows container. The default is Linux, so you don't need to specify that if you're running Linux containers. And we can use --cpu and --memory to say how many virtual CPUs we want, the default is one, and how much memory we want, and the default memory is 1.5 GB. We can specify environment variables for our container, and we do that with"-e and then name=value. We can specify the restar- policy, which can be always, never, or onfailure. And finally, there's a group of arguments that are to do with mounting Azure file shares as volumes. We need to specify the credentials in terms of an account name and key, the mount path, where we want to mount the volume to in our container, and the share name on the Azure file share. And we'll actually be seeing most of these options in action in the demos coming up. So let's start off by creating our first ACI container group in our next demo.

Demo: Creating a Ghost Blog
In this demo, we'll use the Azure CLI to create a new container group, and we'll be deploying an instance of Ghost, which is an open-source blogging platform. We'll create a resource group with az group create, then create our container group with az container create. We'll see how to look at the logs with az container logs, and finally, delete everything we created with az group delete. So here I am in a PowerShell command prompt with the Azure CLI installed, and I've already logged in with az login and selected the subscription I want to use. So our first step is to create a resource group that is going to hold our container group. I'm going to name my resource group AciGhostDemo and put it in a location of West Europe, so I'm storing those in variables, and now I create a resource group with the az group create command that we've seen before. And that happens nice and quickly. So next, we're going to create our first container group. I'm going to give this a name of ghost-blog1, and now I'm going to use the az container create command. I'm specifying the resource group we're putting it in, the container group name. I'm specifying what image I want to use for this container, and in this case, I'm using the official image of the Ghost blogging platform that's available on Docker Hub. I need to specify which ports I want to open, and Ghost uses port 2368 by default, so I need to open that. I'm saying that I do want a public IP address and that I want the DNS name label prefix to be ghostaci1. And this command might take a minute or so complete depending on the size of the container image, and it will show us the status of the container. Now here we can see that our provisioning state is showing as succeeded, which means that we've created this container, and it's up and running. But sometimes you might see pending, in which case you'd need to wait a bit longer before you can use it. We can also see the IP address that we have been assigned in the output, as well as the fully qualified domain name, which is ghostaci1.westeurope .azurecontainer .io. So let's see if our container is actually running by visiting that domain name on port 2368 in a web browser. And, as you can see, our container running the ghost blogging software is up and running. And if we visit the admin page at /ghost, then we could run through the setup wizard to get this blog configured the way we want. And so, as you can see, it's an incredibly fast way to get up and running, and it would be brilliant if we were just wanting to try out Ghost to see if it met our needs. Obviously, if we then decided that we wanted to run it permanently, then we might find a cheaper way of hosting it than running it on Azure Container Instances, but this is a great way of quickly getting a disposable instance that we can play with. Let's have a look at how we can access the logs. If I say az container logs and specify the containerGroupName and the resourceGroup name, it will show me all the log output that's been produced by this container. And apologies that my PowerShell command prompt doesn't seem to be able to display all the characters in these logs properly. So now we've finished our experiment with Ghost, let's clean up, and I'm going to do that by deleting the whole resource group that I created. So we can do that with az group delete, specifying the name of the resource group and the -y flag to say, yes, I really am sure that I want to delete this. And once this command completes, that means everything's gone, and we're no longer paying for our Ghost blog container.

Demo: Using ACR and Mounting Volumes
In this demo, we're going to create another ACI container group, but this time we're going to be using a Docker image from an Azure Container Registry. And we're also going to mount an Azure file share as a volume, and we'll see along the way how to use the az container exec command to execute a shell command within our container. So here we are at the PowerShell prompt again, and we're actually going to be using the Azure Container Registry that we created in the last module. If you remember, we pushed a sample web app application into that container registry. Now, the name of that container registry was Pluralsightacr, so I'll store that in a variable. And if we run the az acr repository list command, we can see there's our sample web app image that we pushed in the last module, and you might remember that it had the v2 tag. Now we will be needing the logging credentials for the ACR repository, so first I'll get hold of the name of the login server with az acr show, which we also saw in our last module, and that gives me a value of Pluralsightacr.azurecr .io. And I'm also going to need the password, and we can get that in a very similar way with az acr credential show, and I'm going to query for the value of the first password. And by the way, this is why in our last module we needed the admin-enabled flag set to true when we created our Azure Container Registry, as it's that flag that enabled us to request this password. So we've got all the details we need from the Azure Container Registry, but for this demo, I'm also going to need a storage account because we want to mount a file share to our container group. And so let's start off by creating a resource group to put that in. I'm going to give my group a name of AciPrivateRegistryDemo and a location of westeurope, and we'll create a resource group with az group create, again, a command that hopefully you're feeling quite familiar with by now. I also need a name for my storage account, which has to be unique, so I'll just include a random number in that name. And we're going to create a storage account with the az storage account create command, specifying the resource group we want to put it in, the name of the storage account, and I'm choosing the Standard_LRS pricing tier. Now, I do need to get hold of this storage account connection string, and I can do this by using the az storage account show-connection-string command and specifying a query that will just pick out the connection string property. And so that connection string is currently stored in this variable, but I need it as an environment variable, and the way I can do that in PowerShell is like this: specifying the name of the environment variable that I want to set and its value. And the reason that I needed to set that environment variable is that we're going to be running another az command that is expecting that environment variable, and that's the command that creates the file share within this storage account. So the name of the share that I'm going to create is going to be acishare, and then I create it with az storage share create and specify the name of the share. And it's this command that's making use of that environment variable that we just set. Finally, I do need to get one more bit of information from this storage account, and that's the storage key because we're going to need that when we create our container group. And I can get hold of that with the az storage account keys list command, again with a query string that picks out just the bit that we're interested in. Okay, so that was a fair amount of setup to create that storage account and the file share, and I suppose I could've just done that all beforehand, but I thought you might be interested in seeing how we can use the Azure CLI to set all of that up as well. So, we've got an Azure Container Registry that contains our Docker image and we've got an Azure storage account that contains a file share, and that means we're ready to create our ACI container group. And I'm going to give this container group a name of aci-acr, and we're going to create it with the az container create command just like we did in our previous demo. But there's a lot of parameters here, so let me just walk you very quickly through what each one does. We're specifying the resource group it's going in, the name of the container group. We're specifying what image we want to run, which is an image that's found on our private Azure Container Registry server, so it has the prefix of the server name and then /samplewebapp, and we'd tagged it v2. I'm specifying how much CPU and memory I want, and then because we're using an image that's on a private ACR container, I'm going to need to provide a registry username and password that can be used to access that Docker image. Then the next four arguments are setting up our file share as a volume. I need to specify which storage account name the file share is in, what the storage account key is, what the name of the share is, and then where I want to mount it in my container. And I'm saying I want to mount it in the /home folder. Next, I actually want to set some environment variables on this container. I'm setting the TestSetting variable to say FromAzCli, and I'm setting the TestFileLocation environment variable to be /home/ message.txt. And that's because our sample web app application makes use of these environment variables, which we'll see in a moment. Finally, I'm saying that I want the DNS name prefix to be aciacr1, and I want to expose port 80. Okay, so I appreciate that is a lot of settings, but really it's just this one single command that allows us to create that container from the private ACR registry and with a file share mounted as a volume all in one step. And again, when we run this, this will take a few minutes to complete, so I'll fast-forward to when it's finished creating the container. But once our container is up and running, we can visit it in a web page. And we can find out what the domain name we need to visit is with the az container show command, like this. And so now when we visit that domain name, we can see that my test website is running in its container, and we can see that the environment variable that we set up at the command line is showing here. And it's also saying that the file at the test file location we specified doesn't exist, and that's because we pointed at a message.txt file in the Azure file share, and there isn't one. It's just currently an empty file share. So let's see if we can actually write into our container's file share. And we can do that by using the az container exec command, which is similar to docker exec that we've seen earlier in this course. So let's execute a simple shell command against our container. Once we're in, we're now actually able to type commands and run them against our container. So if I say echo "hello" to /home/ message.txt, that's going to create a file in our mounted volume. And so I'll exit out of the shell session, and now if we go back to our browser and we reload the web page, sure enough, we can see that we're now able to read that file that we've just written into the file share. And if we want to prove to ourselves that that file really is in the file share and not just in the container, we can use the az storage file list command to see what files are in our share. And so here we can see that the message.txt file we created inside of our shell session is safely stored inside our file share. And that means that we could delete the container group and stop paying for it, but not lose the data that was in our file share, if we wanted to. But now I'm done with this demo, I'm going to delete both the container and the file share by deleting the resource group that contains them both. And I do that, once again, with az group delete, specifying the name of the resource group and the -y flag. And of course, our container registry is in another resource group, so that's not going to get deleted.

Module Summary
In this module, we learned how to run containers on Azure Container Instances. We saw that Azure Container Instances offers a serverless platform that makes it really quick and easy to get a container running in Azure with minimal effort. And we saw that its per-second pricing model makes it ideal for short-lived containerized workloads. We learned how we can use the Azure CLI to create a container instance with the az container create command, examine its logs with az container logs, and run interactive commands against it with az container exec. And some additional features of Azure Container Instances that we used are the ability to mount Azure file shares as volumes, configure environment variables, and use Docker images that are stored in private Azure Container Registries. And of course, there's much more that you can do with Azure Container Instances, so if you want a more in-depth look and lots more examples, then do check out my Azure Container Instances: Getting Started course that's also available here on Pluralsight. In out next module, we're going to look at another way of running containers in Azure with Web App for Containers.

Running Containers on Web App for Containers
Module Introduction
Hi, Mark Heath here, and in this module, we're going to learn how we can run containers on the Web App for Containers service. And Web App for Containers is the second of the four ways of hosting containers in Azure that we're going to be exploring in this course. Web App for Containers is part of the Azure App Service platform, so we'll start off by learning a little bit about what App Service is and what features it offers. And then we'll discuss why we might want to use Web App for Containers and what would be good use cases for it. And we'll see a demo of deploying WordPress to Web App for Containers. We'll see how to scale out to multiple instances of the web server, and we'll see how to configure continuous deployment for our containers. So let's get started.

Azure App Service Overview
Azure App Service is a rich and powerful platform designed to make hosting web applications as simple as possible, and you can use it to host regular websites or web APIs, as well as to implement backends for your mobile apps. One of the most attractive things about Azure App Service is that it comes with many features that are ideally suited for hosting web applications. For example, you can configure custom domains and SSL certificates for your sites. You can scale up to a more powerful web server or scale out to multiple instances of your web server. You can configure staging slots to try out a new version of your application, and when you're ready, swap the staging version into production. It's really easy to manage environment variables and connection strings for your web applications. There are rich monitoring options in the portal, allowing you to easily see if lots of users are getting 400 or 500 errors. And there are lots of options to integrate with a CI/CD pipeline, so it's easy to automate deployment of new versions of your web app. And there are many security features such as configuring IP address whitelisting or requiring Azure AD authentication on all incoming requests. And there's lots more as well like easily enabling a CDN for your web application. So App Service is a Platform as a Service offering design so you can simply provide your web app and then configure as many of those features as you need for your scenario. With App Service, you start off by creating an App Service plan. And the App Service plan is what you pay for, and you can host multiple web apps on the same App Service plan. And there are several pricing tiers such as Basic, Standard, and Premium, each of which brings additional functionality and performance. Now, App Service doesn't require you to containerize your web apps. Out of the box, it already supports many of the most popular web development frameworks. So, as you'd expect, being from Microsoft, there's great support for ASP.NET applications, but you can also run Node.js, Java, PHP, and Python web apps. However, a while ago they added the ability to provide your web application as a container with Linux as the underlying hosting operating system, and more recently, they've also opened up the possibility of running Windows containers on App Service, although that's still in preview at the time of recording. And Microsoft refers to the feature enabling you to host containers on App Service as Web App for Containers. Web App for Containers supports most, but not all of the features offered by regular web apps on App Service. So, for example, you can still configure custom domains and SSL certificates, you can scale up to multiple servers, and you can use the staging slots feature to test the next version of your site in staging before pushing it live. So let's talk next about why you might choose to use a container for your web app on App Service.

Why Web App for Containers?
So if Azure App Service is already able to run web apps built with a whole host of common web frameworks, why would you want to use Web App for Containers? Well, I can think of a few reasons. First of all, you might just really like the consistent deployment model that containerizing applications offers. Maybe you're already using Docker for all the other components in your application, and so you might want to keep things the same way for your web apps. And by containerizing them, you've got the flexibility to easily host them elsewhere if you want to. Secondly, although App Services supports many common web frameworks, it doesn't support everything, so providing your application as a container means that you can use any web development framework that can be hosted in a container. Third, even if you are using something like ASP.NET, because App Service is a Platform as a Service offering, you don't have direct control over exactly what version of the .NET Framework is installed on the host service. For example, at the time I'm recording this, Azure App Service has .NET 4.7 .1 installed by default, but next month it's going to be upgraded to .NET 4.7 .2, but I've got no control over that. In most cases, that's a good thing. The framework is getting patched automatically, and I don't need to do anything special to enable it. But sometimes we do want control over the exact versions of the frameworks that our applications are running on, and containerizing them gives us that control. Another interesting capability that Web App for Containers offers is for your web app to consist of multiple containers, and you can use the Docker Compose or Kubernetes configuration file formats to define the containers that make up your application. Another benefit of containerized web apps is that Azure App Service puts applications into a sandboxed environment to protect other web apps running on the same host, meaning that there are some restrictions on what you can do in a regular web app. For example, certain Windows APIs aren't allowed to be called. However, if you provide your web app as a container, then Docker is providing strong isolation, and so your web app is allowed to do anything that you can do inside a container. Finally, although the regular App Service offers loads of great CI/CD options, using Web App for Containers opens up an interesting possibility that you can trigger an automated deployment whenever a new version of an image is pushed to your container registry. You can set it up to pull that new image and run it in a staging slot, allowing you to swap slots once you're happy that the updated version is working correctly. So there are plenty of great reasons for choosing Web App for Containers in preference to regular App Service web apps. And of course, the main reason you'd be hosting your containers on App Service in the first place is because you're creating a web application or a web API and you want to take advantage of the many great web hosting-related features that we looked at earlier. So now we've got an idea of what Web App for Containers is and why we might use it, let's see it in action.

Demo: Running WordPress on Web App for Containers
[Autogenerated] in this demo, we're going to deploy a WordPress blawg to weapon for containers. We'll start off by creating an APP service plan. We'll also need to create an azure my sequel database, which our WordPress container is going to talk to. And then we'll create a Web app and configure it to use a WordPress container image from Dhaka hub. So here I am in a power shell prompt, and I'm going to be using the ashes. See Ally again. And so, as with all our demos, the first step is going to be to create a resource group. I'm going to give it a name off WordPress app service and put it in West Europe and use the ese group Create command to create it. Next, we're going to create our APP service plan that will host our Web app for containers. Remember, it's the APP service plan that we're paying for and we can host multiple Web apps on a single app service plan. I'm going to give my plan and name and then I can use the a c. AP Service Plan Create Command to create the new plan, providing the plan name, Resource group and location, but also specifying that I want a Lennox hosting plan because we're going to host linens containers. And I've said that I want the S one pricing tier, which is the standard pricing tier, and opens up a broad set of capabilities. Once our service plan has been created, we're going to create on my secret database. I'm going to need a name for my my sequel server, which does have to be unique. Cross is your and I've also defined the user name and password I want to use for this server. I can create them. My sequel server with the ese, my sequel server, Create Command. And for this demo to work, I do need to have this SSL enforcement disabled flag set I love also specified the pricing tier and my sequel version that I want to use. But don't worry too much about the settings for this command, though our main focus in this module is on the Web app, which we're going to get on to in a moment once we've created in my sequel server. The other thing I need to do is to open a firewall roll toe, allow my APP service to talk to it. And a simple way that we can do this in Asia is to specify a start and end i p address of 0000 which is a special value, meaning that we want to allow all traffic coming from within. Asha, if you were doing this in a real production environment, then you'd explicitly set the I P addresses that your APP service plan was using for greater security. Now we're ready to create our Web app itself. I'm going to choose an apt name which again has to be unique across is your cause. It's going to form part of a domain name. And I'll also stole the doctor image name that we want to use in a variable here. I'm just saying wordpress, which means used the official word press image off of Dhaka hub and then we create the web app with a Z Weber create, specifying the app name and resource grape app service plan we're going to use and the doc image that we want it to be based on. And once this is completed, our where that will be up and running. But the WordPress container expects um environment variables in order to know how to connect to the mice equal database. And we've not set those up yet. And the way that APP service lets you specify environment variables is by customizing the application settings for your Web app. So let's configure those. Now we need to know the domain name that are my sequel server is using, and I can get that with the ese. My sequel server Show Command, as you can see, are my sequel Server is hosted here at my sequel, W. P. A s deep, that my sequel dot database doctors your dot com and to set the APP settings for a Web app. I can use the ese Web app. Come pick upsetting set command and then specify all of the environment variables I want to set up, which for the WordPress container need to bay WordPress Deby host WordPress Deby user and would press day be password. And so that's our Web app, container up and running with the correct come pick. Let's check that it really is working every app service, Web app as a domain name that's based on the at name, followed by dot as your websites dot net. We can get hold of this if we want with the ese where about show command like this? And so now if we visit this domain name the first time we do this, it will take a little while to start up because WordPress needs to create the tables in the mice equal database. But once we've done so, we can see that the WordPress installation screen is present. And so if I want to, I can follow this wizard through creating a user and customizing a post on my WordPress block. So hopefully that demonstrates how straightforward it is to get a container running on Web app for containers. Let's see next how we can scale out to multiple service.

Demo: Scaling out and Exploring the Azure Portal
In this demo, we'll see how we can scale our App Service plan to multiple instances, and we'll also look at our web app in the Azure portal and see some of the configuration and monitoring options available to us. Scaling out an App Service plan is really easy to do either from the Azure CLI or from the portal. At the Azure CLI, all we need to do is use the az appservice plan update command, passing in the App Service plan name and resource group, and set the number of workers to whatever we want. Here I'm requesting three workers. And this will scale out to three instances of our web server. Obviously, this now means we're going to be paying three times as much. But notice this is done at the App Service plan level, not at the web app level, so all web apps hosted on this App Service plan will now also be using three workers. And another thing that's really important is that you only do this with stateless web applications. And that's why I hosted our MySQL database separately as a MySQL service rather than putting it in a container, and that means that all three instances of our web application will be sharing a single MySQL database. Let's take a quick look now in the portal and see what options that has for us to configure and monitor this application further. So here I am in the Azure portal, and I've already navigated to the web app that we just created. On the Overview tab, we can see various information about our web app such as its URL, the App Service plan that it's on, and the fact that it's running, and we can restart it from here if we want to. On the left, there are many other pages that we can visit to explore and configure our web application. For example, in Application settings, we can configure some general options like Always On or what version of HTTP we're using, as well as managing the application settings that hold the container environment variables, such as the name of the MySQL server that we're connecting to. Under Container settings, we can see details of the container image that we've chosen. And we're just using a Public WordPress image from Docker Hub, but you can see we could also come in here and configure it to use an image from Azure Container Registry or any private registry. You'll also notice some preview options here, which allow us to use either the Docker Compose or Kubernetes YAML formats to define web apps that use more than one container. And another great thing about this view is that it shows us the log app for our container, which is great for troubleshooting. And there are loads more settings available. For example, we can turn on a layer of authentication around all incoming traffic, or we can set up a custom domain name that we've bought, and we can configure an SSL certificate for that custom domain name. If we look at the Scale out App, we can see that our App Service plan is currently scaled out to three instances, and here we could manually adjust that number or even set up some autoscale rules. And you might notice that some of the options in this left-hand panel are grayed out, and that's because Web App for Containers doesn't support everything that regular App Services can do, but as you can see, the vast majority of features are available when you're running your web apps as containers.

Demo: Configuring Continuous Delivery with ACR and Swapping Slots
In this demo, we'll see how we can configure continuous delivery for our web app. We'll start off by creating a new web app that uses a container hosted in an Azure Container Registry. Then we'll create a staging slot for our web app and enable continuous delivery. Then we'll configure a webhook so that whenever we push a new image to our Azure Container Registry our slot will get automatically updated. And finally, we'll see how to perform a slot swap. So, first of all, let's create a new resourceGroup with the name cicdappservice and the location of westeurope, and we're using az group create as usual. And just like we did in our previous demo, we'll also create an App Service plan. I'll call this plan cicdappservice, and we'll create it with az appservice plan create with the --is-linux flag set and using the Standard pricing tier, which we do need to make use of the deployment slots feature. Next, let's create a web app. We'll give it the name cicd-Pluralsight and create it with az webapp create. Now, unfortunately, there is a limitation at the moment with this command, and that's that the parameters don't allow us to pick an image hosted on a private container registry, and that is an issue that the CLI team do claim that they're planning to fix. So we need to temporarily work around, and what I'm doing is I'm simply picking a publicly available Docker image that we're going to use temporarily. I'll use one called scratch, which you might know is the smallest Docker image available, but it doesn't actually do anything. Now, once we've created the web app, we're going to update its configuration to point to the actual image we want to use in our private Azure Container Registry. And the image we're going to use is the sample web app image that we've used before in this course, and I've still got the Azure Container Registry that we created earlier on called Pluralsightacr that holds that image. And we do need the credentials for the Azure Container Registry, and I've shown you the commands to do this a few times before, so I won't explain them in detail again, but we're going to get the $acrLoginServer name, we're going to get the $acrUserName, and we're going to get the $acrPassword. Now we're ready to configure that web app that we created to use the image from the Azure Container Registry. And we do that with az webapp config container set, specifying the web $appName and $resourceGroup, the name of the container image that we want to use. And remember that when we push images to an Azure Container Registry, they're prefixed with the Azure Container Registry login server name, and then a slash, and then the image name, and I'm saying I want to use whatever version of this image is tagged as latest. And my Azure Container Registry already has an image that is tagged like this. Finally, we're also providing the Azure Container Registry's URL, username, and password. And so once this command completes, our web app will be pulling down that image from the Azure Container Registry and running it. So let's find out what domain our web app is hosted on using the az webapp show command again, and so let's visit that domain in the browser. It might take a minute or two to come online if it's still pulling down the image, but once it's completed, we can see here's our sample web app, and it's running correctly. So now let's create a staging slot. We can do that with the az webapp deployment slot create command, specifying the name of the web app and the resource group that it's in. We're giving our slot a name of staging, and we're saying that we want to clone the settings for this slot from the main web app that we created. So initially, the staging slot will be using exactly the same container image as our main web app. And again, we can find out the domain that our staging slot is hosted on with the az webapp show command, and you can see that it's exactly the same as the main domain name except it has a -staging suffix in the subdomain. So let's just check that's worked by visiting the staging slot in a browser, and here we can see there's another container running our sample web app. So now we're ready to enable continuous delivery. And we could do that for our main web app, but I'm just going to do it for the staging slot. And that means that I can manually test a new version before performing a swap to production. And so we're going to use the az webapp deployment container config command, specifying that we're setting up the staging slot and that we're enabling continuous delivery with the --enable-cd flag. As you can see, when this command completes, we're provided a webhook URL. And we're going to need to give this to our Azure Container Registry so that it can notify our web app whenever someone pushes a new image to that registry. And so I'm going to use the az webapp deployment container show-cd-url command to get hold of that webhook URL and store it in a variable. And now we can configure our Azure Container Registry to call that webhook whenever a new image is pushed to the registry with the az acr webhook create command, specifying the acr registry name, the name we're giving to this webhook, and saying that we only need the webhook to be called when an image is pushed, and, of course, passing in the actual webhook URL. Right, so now we've got continuous delivery set up, let's see what happens when we push a new version of our Docker image to our container registry. And I've already made a small update to my sample web app. I've added the text UDPATED for CI/CD to the home page. And I've already performed a Docker build of this new version locally and tagged it with the correct tag, so all I need to do now is use the docker push command to push that image up to my Azure Container Registry. And the reason I'm able to push to the Azure Container Registry is, like we saw earlier in this course, I've already called the az acr login command to give Docker the credentials it needs to push to that container registry. Once the image has been pushed to the Azure Container Registry, our webhook will be called, and that will cause our staging slot to be updated to use the new version of the sample web app container image. Now, again, it might take a minute or so for this to take effect, but once we visit our staging slots and we refresh the page, we should see the updated version with the additional text. And at this point, in a real-world scenario, we might want to do a bit of manual testing just to verify that we're happy with the new version in the staging slot before we swap it to production. And a slot swap is performed with the az webapp deployment slot swap command, specifying the web app we want to swap, the slot we're swapping from, and the slot we're swapping to, and by default, the main slot is called production. And once the slot swap completes, we can now go and visit the home page of our main application. If we refresh, we can see that now we're looking at the updated container. And if I go back to the staging slot's domain name and refresh that, we can see that now it has the container that was in the production slot. And the advantage of that is that we could swap back if we discover a problem. So, as you can see, it's not too difficult to configure continuous delivery and slot swapping, and it allows some powerful and flexible options for you to manage continuous delivery of your web applications while requiring manual approval if that's what you need. So let's clean up what we've done in this demo. First of all, I'll delete the resource group that has our web app and App Service plan in it with az group delete. And we did those that create a webhook as well, which is in our Azure Container Registry. And we haven't deleted the Azure Container Registry, so let's just delete the webhook with az acr webhook delete, specifying the name of the registry and the name of the webhook that we created.

Module Summary
In this module, we learned how we can use Azure Web App for Containers to host our containers. We saw that Azure App Service is a feature-rich platform with lots of capabilities well suited to hosting web applications. We learned that it can host both Linux and Windows containers and even has support for multi-container applications. And in our demos, we used several Azure CLI commands, including az appservice plan create to create a new App Service plan and az webapp create to create a new web application. We updated the environment variables for our container with az webapp config appsettings set, and we scaled to multiple web servers with az appservice plan update. We created a staging slot with az webapp deployment slot create and configured it to access a container image hosted in an Azure Container Registry with az webapp deployment container config. Finally, we swapped the production and staging slots with az webapp deployment slot swap. And if you'd like to try this for yourself, all the commands that we ran through are available in script files in the course download materials. In our next module, let's see how we can run containers on Azure Service Fabric.

Running Containers on Azure Service Fabric
The Challenges of Microservices
Hi, my name's Mark Heath, and in this module, we'll learn about how to run containers on Azure Service Fabric. Now, so far in this course, we've looked at two out of four ways of running containers in Azure. We saw that Azure Container Instances is great for quickly running short-lived container workloads and that Web App for Containers is great for hosting a web application or API as a container. But when you start using containers to build larger applications, you often end up creating several containerized microservices, and Azure Service Fabric is the first of two offerings in Azure that we're going to look at that are ideal for hosting containerized microservices applications. And there are many benefits of using a microservices architecture, but they can also bring significant challenges as well. For example, how do we deploy all of our containers? How do we monitor their health? What if we want to host multiple instances of one of these services across multiple nodes in a cluster for high availability? How can our services discover each other and communicate securely? How can we implement zero downtime upgrade strategies like rolling upgrades or Blue-Green swapovers? And how do we handle hardware failure and move our containers to another node if one fails? And these are all the sorts of problems that orchestrators are designed to address, and Azure Service Fabric can be thought of as an orchestrator or a platform for running microservices on. It's actually a very rich and powerful platform, and we won't have time to cover all of its capabilities, but in this module, I want to give you a high-level introduction to what it can do, and we'll also get to see it in action.

Introducing Service Fabric
So what exactly is Service Fabric? Well, Azure Service Fabric is an application platform designed to simplify building and deploying scalable and reliable microservice-based applications. Service Fabric isn't just limited to Azure. It can run on-premises or in other cloud providers, and you can even run it locally on your development machine, which is great for testing purposes. But, of course, it's really easy to set up and run in Azure. With Service Fabric, your services run on a cluster, and they're continually monitored, so if there are any problems, the platform is able to act automatically to resolve the issues, for example, by moving services to another node in the cluster or even by rolling back to a previous version of a service. And Service Fabric offers several different programming models. Your services can be stateless or stateful. Stateful services, which we won't be using in this module, are a very innovative approach that allow you to co-locate compute and data, and this has the benefit of reducing latency and providing resiliency by replicating and persisting your data across several different nodes. But to make use of this stateful service model, you do need to make some changes to the way you write your code and make use of some special APIs known as the reliable collections. But what makes Service Fabric interesting for the purposes of this course is that it also supports stateless services. That is to say ordinary applications such as Web APIs or executables that can be scaled out to multiple instances because they aren't maintaining their own state. And one of the options Service Fabric offers for stateless services is for you to provide those services as containers. Service Fabric can be used to host both Windows and Linux containers, and of the four Azure services that we're exploring in this course, Service Fabric is the most mature platform for hosting Windows containers on, and so in this module, for our demos, we're going to be using Windows containers. A Service Fabric cluster can consist of Windows and Linux nodes, allowing both types of containers to be used together in the same application, and Service Fabric also allows you to constrain the resources, such as RAM and CPU, that are made available to each container. Service Fabric also has support for the popular Docker Compose format, which allows you to define all the containers that make up your application in YAML. And if you're wondering whether Service Fabric is mature, scalable, and reliable enough for you to entrust your mission-critical applications to, it's worth pointing out that Microsoft themselves use Service Fabric as the underlying platform for many of their flagship products, including Cortana, Skype, Cosmos DB, and Power BI. So let's summarize some of the reasons why you might choose Azure Service Fabric. If you have an application that has a microservices architecture and you need orchestration capabilities like service discovery and health monitoring, then it's ideal. It's a great choice if your application uses Windows containers, but you're not limited to them. You can also combine them with Linux containers or those special stateful services that Service Fabric offers. Or maybe you want the flexibility of not only being able to deploy to Azure, but also be able to deploy on-premises or to other cloud providers, or maybe some of the other Service Fabric orchestration features like rolling upgrades and automatic rollbacks would be beneficial for your applications. So there's lots of reasons why you might choose to use Azure Service Fabric. Let's now see some demos of it in action.

Demo: Setting up a Service Fabric Development Environment
Let's see how we can set up our development environment to work with Service Fabric. And there are two main things you'll need to get started. First, if you're using Visual Studio, which I do recommend for Service Fabric development, then you do need to make sure that you have the Service Fabric Tools installed, and this adds templates for you to create new Service Fabric projects. If you didn't select this option when you initially installed Visual Studio, don't worry. You can use the Visual Studio Installer to modify your installation and ensure that you have Azure development selected and, in particular, the Service Fabric Tools option. Once you have the Service Fabric Tools installed, then when you select File, New, Project in Visual Studio, you'll be able to select Service Fabric Application as an option. However, when you to go create the application, you'll see that the dialog which allows you to choose what sort of service you want to create will prompt you to install the Service Fabric SDK as well. And when you click on this link, it will download the Web Platform Installer, and then when you run that, it will automatically select the Service Fabric SDK to be installed. Once that is installed, you'll actually be able to run a Service Fabric cluster locally on your development machine. So that's how you set up your development machine to create Service Fabric apps. Let's see next how we can create a Service Fabric cluster in Azure.

Demo: Creating an Azure Service Fabric Cluster
In this demo, we'll see how we can create a Service Fabric cluster in the Azure portal. So here I am in the Azure portal, and to get started, we'll create a new resource and search for Service Fabric Cluster. We'll select it from this list and click Create. And now we need to give our Service Fabric cluster a name, which does need to be unique across Azure. Next, we need to pick an operating system for our cluster to run. I'm going to be using Windows containers in this demo, so I need to pick a version of Windows that supports containers. And I'll pick the most up-to-date version of Windows that offers container support, which is 1709 here, and hopefully before too long we'll also see Windows Server 2019 as an option in here. I need to provide login credentials for the machines in our cluster, and once I've done that, I'll also create a new resource group to put this cluster in. And I'm happy to use the default location of West Europe. In the next step, I'm going to configure the cluster. A cluster can contain multiple types of nodes; however, for our demo purposes, one node type is just fine. I'll call my node type primary, and I'll need to select the virtual machine size. And I'll just pick the basic D1_V2 Standard, but you can select from a very wide variety of available virtual machines in Azure. We can also choose the scale set capacity, which defaults to 5, and you do need to bear this in mind when you're thinking about the cost of your cluster. Five virtual machines may be fine for a production application, but if you're just experimenting, then it could get a bit expensive if you forget to turn it off when you're finished using it. I also need to specify what ports that I want to open, and so I'm just picking a few here that I know that my application is going to make use of. Next, we move onto security and we need a Key Vault to store a certificate for our cluster. Now the portal will guide you through the process of creating one if you haven't already got one, but I do already have a Key Vault, so I can select that. Now, I can choose to use an existing certificate in my Key Vault if I want to, but I'm going to let it create a new certificate for me with the name sfclustercert1. And once it's created that certificate, I will need to download it onto my development machine in order to be able to publish to that cluster. And this link here gives me a quick way to download it. It jumps me into my Key Vault with the certificate already selected, so I just need to scroll down and click Download as a Certificate. Then, once it's downloaded, I can double-click it to install it onto my local development machine. And I can just accept all the defaults here, and the password for the certificate is blank. Once that certificate is installed, now I'm ready to create the cluster. Although, do notice that I can download an ARM template instead if I'd prefer to use an automated script to deploy a Service Fabric cluster with this configuration. But I'm just going to click Create for now. And this will take a few minutes to complete, so I'm going to fast forward until we're done. And so here we can see the notification telling me that the deployment is complete. And so if I click Go to resource, I can see my Service Fabric cluster with five nodes. And if I take a look at the resource group that my cluster is in, we can see that as well as the Service Fabric cluster, there are a whole bunch of additional resources that may well be familiar to you if you're used to working with virtual machines in Azure. There's a virtual machine scale set, which contains the nodes in our cluster, and there are some storage accounts for the disks. There's a virtual network, a load balancer, and a public IP address. And if we navigate back to the Service Fabric cluster, we can see that we're provided a link to the Service Fabric Explorer for this cluster. Now, if we visit that link in a browser, we will get a warning from Chrome that we can safely ignore because this site is going to be protected by that certificate that we just downloaded. And so when we progress, we're prompted to select that certificate, which I'll do, and this takes us to a dashboard showing us the status of our cluster. We've got five healthy nodes in our cluster, but there are no applications yet. So let's see next how we can publish an application to this cluster.

Demo: Deploying Containers to a Service Fabric Cluster
In this demo, I want to show how we can use Visual Studio to deploy a containerized application to Azure Service Fabric. And this is going to be a fairly high-level overview. We don't have time to go into all the details, but I hope it will be enough to give you a good idea of what's involved in the process. So here I am in Visual Studio, and I've already got the Service Fabric Tools and SDK installed, and I've also already created a new Service Fabric project and added two services to it, both of which are containers. And you can see that I end up with a Service Fabric project in Visual Studio, which contains a whole bunch of XML files, which define the services in my application and how it's deployed. And so let me show you the most important ones. First of all, each service has a service manifest. So, for the SampleWebApp service, this service manifest defines the Docker image that I'm using. In this case, it's a Windows container image that I've uploaded to a Docker Hub, and its name is markheath/samplewebapp, and I've given it the tag win1709. In this manifest file, I can also specify environment variables for my container. I'm setting the TestSetting environment variable to a message we should see when we load our web page, and I'm also asking this container to try to make a request to the back-end service. Azure Service Fabric supports DNS for containers, so I can talk to my back-end service simply using http and then the service name, samplebackend, on the port which it exposes, which is 8081. And so if this works, when we load the web page, we should also see the response from the back-end service. Finally, I can specify an endpoint for this service, and I'm saying that I want it to be accessible on port 8080. Now, if you're paying very careful attention, you might have noticed that when I created my cluster in the last demo I didn't actually open that port, which was a mistake, and I'll be showing you in a moment how to fix that. Now the service manifest for the back-end service is very similar. The image for this service is also on Docker Hub, and it's called markheath/samplebackend, and it's tagged win1709. And by the way, the fact that I've put these images on Docker Hub means that if you download the course sample materials, you'll be able to run this demo yourself without needing to build the container images. Now this service doesn't have any special environment variables, and it exposes port 8081. Now if you're wondering how I created these service manifest XMLs, well, most of the work is done by the tooling. If I right-click on the project and select New Service Fabric Service and then choose Container, here I can specify an image name, and when I click OK, it will generate another service manifest XML file. However, we've already got the services we need, so I'll cancel out of this. The next file we need to look at is the ApplicationManifest, and this defines all the services in our application. Again, the tooling has done most of the hard work for us here. Here I'm saying that my application includes the service that's defined in the sample back-end package folder, and we can update this service manifest version here whenever we update our service definitions. I'm also saying that the container itself is listening on port 80, but that this should be mapped to the back-end endpoint that we defined in the service manifest, which was for port 8081. So traffic coming into our cluster on port 8081 will go to the back-end service whose container is listening on port 80. And if we scroll down, we can see a similar section for the SampleWebApp service pointing to its service manifest and specifying that it too is listening on port 80 and that traffic coming into the SampleWebAppTypeEndpoint, which was port 8080, should be routed to this service. And so those two sections indicated what service manifests we want to make use of in our application, and then there's an additional section here at the bottom that says what services are part of our application. And optionally, we can specify how many instances of each service we want. I've just left this at its default, which will actually mean that we're going to run an instance of each container on each node in our cluster. Finally, let's look at the Cloud publish profile. Visual Studio will let us publish to our local cluster, so we could run this locally on our development machine, but the Cloud.xml file specifies the details of how to connect to our cluster in Azure. As you can see, I've specified the connection endpoint of our cluster on port 19000, which is the port number that we can deploy on, and deployment is secured by that certificate that we've downloaded. So I've put in the thumbprint of that certificate, which you can find either in Key Vault or by using your Windows Certificate Manager. Now, please don't feel intimidated if all of our XML seems a bit much. This was a very quick tour of the configuration for a Service Fabric application. And if you want to go into more depth on this, Pluralsight does have some additional courses on Service Fabric that you can look at to learn more about how these XML manifest files work. But let's see how we can publish this to Service Fabric now. You might notice that there's a PowerShell script that the Visual Studio tooling has created for me. So if we were deploying this as part of a CI/CD process, then we would use that. But if we want to deploy from within Visual Studio, then we can just right-click the project and select Publish. And because we set up that Cloud.xml file with all the relevant settings, it's able to connect to that cluster, and we can simply click Publish. Now, it won't take too long to complete this, although obviously on the cluster it is going to need to download those container images that we're using. And so if we jump now to the Service Fabric Explorer again in a browser, we can see that it's already showing that we've got one application, and pretty soon it will also detect that there are two services. And if we want to, we can drill down and explore our application and nodes in more detail. Now, as I said earlier, I did make one small mistake when I set up my cluster, and that was I intended to open ports 8080 and 8081, but actually, I opened 8081 and 8082. But fortunately, that's easy to fix in the Azure portal. I just had to navigate to my load balancer rules and change the port number from 8082 to 8080. And the load balancer rule has an associated health probe, which was also on port 8082, and I needed to update that to 8080. But once that's fixed, I can now visit my Service Fabric cluster domain name on port 8080, and we can see that our sample web page is loading successfully. It shows the test setting that we defined as an environment variable in the service manifest, and we can also see that this service can communicate with the other service via its DNS name of samplebackend. And if we return to that Service Fabric Explorer now, I can navigate into a node. And so if I drill right down in here, we can actually look at the container logs for a specific instance of one of my services. So here we're looking at the logs for the SampleWebApp service on node 0 of our cluster, and we can see the output from our ASP.NET Core application showing that it was calling the back-end service as part of the page load. Now I appreciate that there was a lot to take in in that demo. Service Fabric is a very powerful platform with a huge range of capabilities, and it does take a bit of time to get familiar with how it all works. However, the good news is that for situations like ours where we're deploying an entirely containerized application to Service Fabric, there's actually a new option on the horizon that will make things a lot simpler, so let's learn about Service Fabric Mesh next.

Introducing Service Fabric Mesh
We've seen that Service Fabric is a very powerful platform capable of running Windows or Linux containers, but it does require you to have a cluster available for you to run your application on. However, there's a new service currently in preview called Service Fabric Mesh, which is based on top of Service Fabric, but offers a greatly simplified model. First of all, it's container focused. Service Fabric can, of course, run containers as we just saw, but it also supports non-containerized services. With Service Fabric Mesh, all of your services are in containers. You can still create those special stateful services if you want to, but you need to create them as containers. Secondly, it's serverless. You don't need to provision any infrastructure in advance. You just need to say for each service in your application what its resource requirements are, so how much RAM and CPU it needs and whether it's a Linux or Windows container. And behind the scenes, there's a Service Fabric cluster managed by Microsoft, which your containers will actually run on, and this makes it much easier to get started. Third, the deployment model is based entirely on Azure Resource Manager, so you can define a Service Fabric Mesh application entirely as a JSON ARM template, which you're probably familiar with if you've done any other Azure deployment work. And there's also a simplified YAML format where each service or component of your application is defined in a YAML file, and then at deployment time those YAML files get turned into an ARM template. Now, because Service Fabric Mesh is still in preview, there are some important caveats that I do need to emphasize. First of all, the deployment file formats, both JSON and YAML and the command-line tool syntax are likely to change. Hopefully not too much, but what you see in the upcoming demos might not be the final format. And also, there are a few key features that we're still waiting to be added to the platform. Now, the Service Fabric Mesh team are working hard at getting out of preview and into public release as soon as possible, but if at the time you're watching this course it's still in preview, you probably want to bear that in mind when considering whether it's an appropriate platform to build a new application on. But having said that, I think Service Fabric Mesh is a really exciting development, and I think for many applications it will make sense to use it in preference to using a regular Service Fabric cluster. So in our next demo, let's see Service Fabric Mesh in action.

Demo: Using Service Fabric Mesh
In this demo, we'll use the Service Fabric Mesh preview to deploy a containerized Windows microservice application that consists of five services, and we'll use the preview mesh extension to the Azure CLI to deploy an ARM template that defines our application, and then we'll also use the CLI to scale one of the services in our application out to multiple instances. Now the application I've chosen to deploy is based on this project in the official dockersamples GitHub repository, and it's a voting application consisting of five containers. It has one website that lets you vote for either cats or dogs, another website that shows you how many people have voted for each option and is live updated with the current total, and the way it works is that when you vote, a queue message is sent, which is received by a container running the open source NATS messaging system, and then another microservice, called the worker service, reads the messages out of the queue and updates a database of votes, and that database is another container running the open source TiDB database. So there are five containers in all. So let's take a look at the ARM template that defines our Service Fabric Mesh application. Now, just like the XML manifests that we saw earlier, this will probably look quite intimidating if you're not familiar with ARM templates. But once again, you don't normally write this all yourself. The tooling can generate these files for you. The only things that you need to fill in are the details that are specific to your application. Now, an ARM template can contain parameters, and I've defined three parameters that govern how many replicas of the vote, result, and worker service that we want, and I've set the default number of replicas to 1. Then, in the resources section of our ARM template, first of all, we define a Service Fabric Mesh application, which has the name votingApp, and it depends on the votingNetwork, which is another resource defined further down in this template. In the properties section, we can see that a Service Fabric Mesh application has an array of services, and we've got five of these. The first is the vote service, which is a Windows service, and its code package defines the container that makes up the service, and we're using the examplevotingapp_vote image from Docker Hub with the dotnet-nanoserver tag, which is a Windows container image. And the vote service exposes an endpoint on port 80. I'm saying that we want half a CPU and 1 GB of memory to be allocated. And the replicaCount is going to be taken from the parameters, and we're saying that this service should be attached to the votingNetwork. Now, the rest of the services are fairly similar in their definition, so let's go through them quite quickly. The result service also uses an image from Docker Hub. It listens on port 20002, and for this container we've defined some environment variables so it knows how to access the database and what port to expose. You might also notice from this connection string that there's a DNS service which allows the services in our application to simply refer to each other by their service name. And the worker service also has an environment variable for the database connection string, and it's based on another image from Docker Hub. If we look at the message-queue service, we can see that it's using the open source NATS messaging system and, again, using a Windows container. It's listening on port 4222. And finally, the database service is using the TiDB open source database, and it's listening on port 4000. Finally, at the bottom of our ARM template, we're defining the voting network, which all of our services are connected to, and this lets us set up the ingress rules for our application, and this defines what public ports we're exposing and which service they should be routed to. So port 8081 will go to the vote service, and port 8082 will go to the result service. Again, please don't worry if that all seems a bit complex. You won't be writing these from scratch yourself. The tooling will help you generate it. And I've made this template available in the course download materials if you'd like to look at it in more detail. Okay, let's see how we can deploy this ARM template with the Azure CLI. Now, because Service Fabric Mesh is in preview, I need to enable an extension to the Azure CLI, which I can do with az extension add and the name mesh. And this will install it for us, although, as you can see, I already had it installed. Next, I'm going to create a resource group with the name ServiceFabricMeshVote, and I'm going to put it in westeurope, which is the data center closest to me that supports Service Fabric Mesh. Now, the deployment step is really easy. The ARM template file that we just looked at is called sfmesh-example-voting- app.json, and we deploy it with az mesh deployment create specifying the name of the resource group we want to put it in and the name of the ARM template file. And that's all we have to do. No need to create a cluster or mess around with certificates. And when this is done, which takes a minute or so, it will show us the public IP address of the application, and if we copy that and visit it in a web browser on port 8081, that will take us to the voting site. And so here we can see our cats and dogs vote, and I'll add my vote here. And this website will actually only allow me to vote once, so offscreen I'll vote again on a couple of other devices, and now if in our web browser we go to port 8082, this will take us to the result website, and we can see the number of votes that there are and what percentage for each option. And so that actually proves that the votes we cast have made it all the way from the vote service to the NATS service, then to the worker service, then to the database service, and finally, here to the results service. So that proves all of our five services are working correctly. Now, if I want to scale a service out, we can do that simply by deploying an updated template, And in fact, because I parameterized the replica count for the worker service, if I want to scale that out to three instances, I can do that very easily with the az mesh deployment create command again. Here I'm specifying exactly the same template, but with the parameters argument I'm overriding the workerReplicaCount to the value of 3. And so this will run, and it will compare the application that's currently deployed with the application defined in this template with these parameters and make any necessary changes. And so when it finishes, we can use the az mesh service list command to view all the services in our voting app application, and we can see that our worker service now has three instances. If we take a look at our Service Fabric Mesh application in the portal, we can see all of our services, and if we want to, we can drill down into an individual service, pick one of the replicas, and go into the code package, and we can examine the container logs if we want to. And so this is a great way to diagnose any problems that you might have with your application running in Service Fabric Mesh. So, as you can see, Service Fabric Mesh greatly simplifies the work involved in deploying a containerized microservice application to Azure. And although it's still in preview, it's definitely worth keeping an eye on because I think it has a very exciting future. Now, of course, if you're following along with these demos, don't forget to delete your Service Fabric Mesh application in the usual way with az group delete when you're done.

Module Summary
In this module, we looked at how we can deploy containers to Service Fabric and Service Fabric Mesh. We saw first of all that Service Fabric is a very powerful orchestration platform that supports both Windows and Linux containers, and you can run a Service Fabric cluster in Azure, as well as on-premises, and even on your local development machine. We saw that Service Fabric applications are defined by service and application manifest XML files, and they can be deployed to a Service Fabric cluster using Visual Studio or PowerShell. We looked at the Service Fabric Explorer dashboard, which is a user interface allowing you to monitor the health and state of your cluster and all the applications that are running on it. And then we peaked into the future with a look at the upcoming Service Fabric Mesh platform, which greatly simplifies the effort involved in deploying containerized microservice applications. It too supports both Windows and Linux containers, and it uses an ARM-based deployment model. It's easily automatable with the Azure CLI, and it operates as a serverless platform, meaning you don't need to provision a cluster in advance. You just pay for the compute that's used by your services. In the next module, we'll see another powerful container orchestration platform, and that's the Azure Kubernetes Service.

Running Containers on Azure Kubernetes Service
Orchestrators
Hi, Mark Heath here, and in this module, we'll be learning about how to run containers on the Azure Kubernetes Service, which is also known as AKS. And this is the final one of four Azure services for hosting containers that we're exploring in this course. Like Azure Service Fabric, Azure Kubernetes Service is an orchestrator, and so in this module, we're going to start off by reminding ourselves what an orchestrator is, we'll learn a few Kubernetes basics, and then we'll learn about what the Azure Kubernetes Service offers on top. Then we'll see a few demos including how we can create an AKS cluster, how we can deploy applications to it with the kubectl command-line tool, and how to scale up both the cluster as a whole and individual microservices within an application hosted on AKS. Now, you might remember at the start of this course we talked about the role of orchestrators. An orchestrator manages a cluster of worker nodes that are each able to run containers, and you provide a description of your application in a declarative format such as a YAML file, and then it's the orchestrator's job to choose which nodes in the cluster to run each of those containers on. And this is known as scheduling, and it can be quite advanced, taking into account the resource requirements of each service and the current resource utilization of each worker node. Orchestrators can also provide health monitoring of your application. They can detect that an individual service needs to be restarted, and if an entire worker node disappears, they can ensure that all the services that it was hosting get moved onto other nodes in the cluster. And orchestrators can help you implement various upgrade strategies such as rolling upgrades, as well as scaling the number of replicas of a particular service. They can also constrain the resources available to containers to stop one rogue container hogging all the resources on a worker node. Orchestrators also have an important role in enabling services to communicate with one another. Given that they can be freely moved around in a cluster between worker nodes, it would be very complicated if you tried to keep track of each container's IP address, and so orchestrators help us solve this problem and greatly simplify service discovery, and they also allow incoming traffic to the cluster to get routed to the correct service no matter which worker node it's hosted on. And of course, Kubernetes can do all of these orchestration tasks and much more. So next, let's learn a few Kubernetes basics.

Kubernetes Basics
Kubernetes is a production-grade container orchestration system originally created by Google, and it's established itself as the leading open-source container orchestration platform. Now there is quite a lot of terminology to learn when you get started with Kubernetes, which can seem overwhelming at first, but once you've used it for a while, it does start to make sense. Now, in this course, we've not got time for a deep dive, but I'm just going to highlight a few of the key Kubernetes resources and concepts that you'll need to know about for this course. First of all, a Kubernetes cluster consists of master and worker nodes. The master nodes, which are also known as the control plane, are like the brain of the orchestrator. They decide what worker nodes each container should run on, and this is known as scheduling. The worker nodes are much simpler. They're just the nodes that are actually running your containers. Kubectl, or kube control, it can be pronounced in a few different ways, is the main command-line tool that you can use to manage your cluster and the resources in it, and we'll be using it later on in this module. A Pod is made up of one or more containers, and it's the smallest unit that Kubernetes manages. Pods are not intended to be long-lived. They might be created on one worker node and then disposed and get recreated on another worker node. A ReplicaSet is used to define how many instances of a Pod should be running in your cluster, and you can deploy a ReplicaSet with a Deployment, which is the recommended way of running code on Kubernetes. Services are another important concept, and you can think of them as load balances. Say you have a Pod that's running a web API. Then rather than trying to send traffic directly to that Pod, which might move around, instead, you communicate with the Service. The Service then routes traffic to that Pod or to one of its replicas if you have several instances. Namespaces allow you to isolate your microservice applications from one another. So maybe you've got a large Kubernetes cluster that you intend to use to host several different applications on or maybe even several versions of the same application. Then you'd use namespaces to isolate them from each other. Kubernetes allows you to describe your application declaratively using a YAML file format, and we'll be seeing some example files later on in this module, but there's an open-source tool called Helm, which has recently emerged as a kind of package manager for Kubernetes. You can define your application with Helm Charts, and these can be made available in a public repository so they can be shared with other developers. And there are publicly available Helm Charts for many open-source software applications like WordPress or Elasticsearch, and using Helm Charts greatly simplifies the effort involved in deploying these applications to a Kubernetes cluster. Okay, that was a very brief and rather simplified introduction to some of the core Kubernetes concepts, but let's move on now to learn about what Azure Kubernetes Service is and what it adds on top of Kubernetes.

Introducing Azure Kubernetes Service
Azure Kubernetes Service is a managed Kubernetes cluster running in Azure. It takes away almost all of the hard work of configuring and maintaining a Kubernetes cluster, which is actually not a trivial thing to do. With Azure Kubernetes Service, the master nodes, or control plane, is fully managed. You don't need to install that at all. Even better, you don't even pay for it. It's just included in the cost of the overall cluster. So when you create an AKS cluster, you simply specify how many worker nodes you want. And these are just regular Azure virtual machines, which are configured automatically for you to join your cluster, and so you're just paying for these worker nodes. AKS also simplifies the task of upgrading to a new version of Kubernetes, which can be quite a complex process. All you need to do is choose the time that you want to perform the upgrade. And AKS doesn't use a special Azure fork of Kubernetes. The software running in your cluster is exactly the same as if you installed Kubernetes yourself, and this means you can use all the exact same tooling that you can use for regular Kubernetes with an AKS cluster. And of course, if you want to, you can configure AKS to take advantage of other Azure services, so you can easily connect it up to Azure Monitor to get rich logging and diagnostic monitoring capabilities, and you can use Azure file shares or disks to mount persistent volumes. You can secure your AKS cluster using RBAC, for role-based access control, to Kubernetes resources and namespaces, and you can integrate with Azure Active Directory to make it really easy to grant your Active Directory users and groups access to specific resources. And you can also deploy your AKS cluster into an existing virtual network. You can integrate AKS with Azure Container Instances, and this allows you to present Azure Container Instances as a virtual worker node in your cluster, which allows you have serverless elastic scale. And this also opens the door to you running Windows containers on AKS, and it's expected that in the near future AKS will directly support running Windows containers. Finally, another really exciting Azure service that runs on top of AKS is Azure Dev Spaces, and this makes a Kubernetes cluster really easy to use from development. From within tooling like Visual Studio or Visual Studio Code, you can choose to run your containerized application directly on an AKS cluster and debug into it. And Azure Dev Spaces also makes it possible for you to have some microservices hosted on the cluster that are shared with other members of your development team, but for you to replace individual containers with ones that you're personally working on that are only visible to you. So, in short, Azure Kubernetes Service makes it really easy to take advantage of all the power and capabilities of Kubernetes in Azure, but it optionally also adds even more value by integrating with other Azure services. So let's see next how we can create an AKS cluster.

Demo: Creating an AKS Cluster
In this demo, I'm going to create an AKS cluster in the Azure portal, but then I'll also show you the commands to do the same thing with the Azure CLI, and we're also going to get to see the kubectl command in action. So here we are in the Azure portal, and we'll select the plus icon in the top left to create a new resource. I'll type Kubernetes into the search box, and we'll pick the Kubernetes Service from the list of available options and then say that we want to create one. Now, there are several pages of settings that we'll need to go through, but for most of this, we can just accept the defaults. First, I'm going to put my cluster into a new resource group, which I'll call AksDemo1, and I'm going to give my cluster the name aksdemocluster1. Now I'm happy with the location that it's chosen for me, and the default Kubernetes version is also fine. It does also require me to specify a DNS name prefix, so I'll use aksdemocluster1 again. Next, I can choose the size of virtual machines I want. Here it's picked out a Standard DS2 v2, which has 2 cores and 7 GB of memory, and I'm going to start with the default node count of 3. Remember, we can easily add or remove nodes from this cluster later. On the next screen, I can configure some security-related settings. I'll let it automatically create a service principal, and I can optionally enable role-based access control here if I want, although I don't really need it for this demo, so I'll leave it off. In the next screen, I can set up some networking-related features. Again, the defaults are fine for our needs here, but you'll notice that if I choose Advanced networking, then I can set it up to join a virtual network, but I'll switch it back to Basic for now. On the Monitoring tab, I can configure it to integrate with Azure Monitor, which will give us lots of diagnostic and monitoring information about what's going on in our cluster. And we don't have any need to add tags, so let's proceed to the final screen where we can review all the settings we've made, and when we're ready to create it, we just click Create. Notice though that I can choose to download an ARM template instead, and that would be a good choice if I wanted to automate the creation of a cluster with these settings. But I'll just click Create, and this is going to take several minutes to complete. And the portal helpfully shows us a screen with the status of the deployment, and I'm just going to fast-forward to when it's completed. So, we can see here that our cluster is now provisioned, and if I navigate into the cluster, it shows me some basic information like the version of Kubernetes it's running, the API server address, current number of CPU cores, and the amount of memory. And there's a link here that will show me what CLI commands I need to run to connect to the Kubernetes dashboard, and I'll be showing you how to do that later. And here's where we'd go to view the logs and monitoring information that we enabled when we created our cluster. If we visit the Scale tab, this is where we'd go to scale a cluster up or down to a different number of nodes. So we've successfully created an AKS cluster in the portal, but I also want to show you how we could do this from the command line. It's actually very easy to do, and we'll be doing all the rest of the demos in this module from the command line. So here I am in a PowerShell prompt, and I've got the Azure CLI installed, and I'm logged in. I'm going to create a new resource group with the name AKSDemo2, and we'll use the az group create command to create that resource group in West Europe. I'm going to call this cluster MarkAks and create it with the az aks create command, specifying the resource group, the cluster name, the starting node count, which I'm going to set to 1 here because we're going to scale this cluster up to three nodes later, and I will also ask it to generate SSH keys that we can use to securely manage this cluster. Now, again, this takes a few minutes to complete, but once it's done, we can use the kubectl command-line tool to explore it. Now, if you've installed a recent version of Docker for Windows, you might already have kubectl available, and you can test that by running the kubectl version command, like this. By the way, don't worry about the error that it can't connect to a cluster. We're going to fix that in a minute. Now, if you've not got kubectl installed, the Azure CLI actually provides a helper method that will download it for you, and that's simply az aks install-cli. But I've already got it, so I don't need to do that. Next, we need to set up kubectl to communicate with our AKS cluster, and we can do that with az aks get-credentials, specifying the resource group and the cluster name. And this updates our local kube config file, and that means that now if we issue the kubectl get nodes command, it will show us that we've got one node in our cluster.

Demo: Running an Application on AKS
So in our next demo, let's deploy an application to this cluster. In this demo, we'll deploy a basic application to our AKS cluster using the kubectl command-line tool. We'll start off by looking at the YAML that defines our application, and then we'll use kubectl apply to deploy the application, and we'll also see how to view the container logs. So here we're looking at a YAML file that defines our application, and it's just got two microservices, which are the samplewebapp and samplebackend that I've been using throughout this course. And this YAML file has got four sections in it defining the four Kubernetes resources that we need. We need to define a Deployment and a Service for each of our two microservices. So first, we're looking at the samplebackend deployment. We're just asking for one replica, and the container image is a public image on Docker Hub called markheath/samplebackend, tag Linux, and this container exposes port 80. But there's also a Service defined for samplebackend, which says that we're exposing this service for incoming traffic on port 8081, and that's going to map to port 80 on the container itself. However, this service won't be publicly available. It's just going to be accessible from within the cluster. Next up is the Deployment for samplewebapp, and this is also using a public image from Docker Hub called markheath/samplewebapp, tag Linux. It too is listening on port 80, and I've set up some environment variables, which we're going to see when we run the application. It should show this message, and it should also try to call the backend service via this URI. And it can do that because Kubernetes will route the hostname of samplebackend on port 8081 to our backend service. Finally, we have a Service definition for samplewebapp, and this one has got a type of LoadBalancer, which means that we're going to expose a public IP address to allow incoming traffic. We're going to be listening on port 8080, and that's going to get mapped to port 80 on the samplewebapp deployment. Again, don't worry too much if the syntax of these files seems a little bit confusing. It does become familiar with practice, and very often you won't have to write all this from scratch. You can use tooling to help you, or you can base your work on other example YAML files. Okay, so let's deploy this application to our cluster, and we can do that with the kubectl apply command, specifying a file name of sample- app.yaml. And when we run this, it will see if any of the resources defined in the YAML file already exist, and if they don't, it creates them. And this happens very quickly. It sees that we need to create two deployments and two services. Now, obviously there will still be a bit of time before those services are running because the nodes in our cluster need to go off to Docker Hub to pull down those container images and start running them. Now, to be able to access our samplewebapp service, we're going to need its external IP address, and it doesn't initially have one. However, we can use the kubectl get service command, asking it for details of the samplewebapp service and pass the --watch flag, and this will basically wait until the external IP address becomes available and show it to us. And so if we wait a bit, once this is completed and shows us that IP address, we can launch a web browser, connecting to the IP address of the service on port 8080, which was the publicly exposed port for the samplewebapp service, and we can see that our samplewebapp is indeed up and running on our AKS cluster. It's showing the hello from Kubernetes message that we defined as an environment variable in our YAML file, and we can also see that it's successfully called through to the backend service using the DNS name for the samplebackend host service on port 8081. If we go back to the command line and call the kubectl get pod command, it shows us the two pods that are running in our cluster. Currently there's one each for the webapp and the backend, and I can use the name of one of those pods with the kubectl logs command to view the log output from the container in that pod, like this.

Demo: Scaling AKS
In this demo, we'll see how easy it is to scale with Azure Kubernetes Service. We'll scale our cluster from one to three nodes, and then I'll deploy a new application, and we'll also scale one of the microservices in that application from one to three replicas. And I'll also show you how we can launch the Kubernetes dashboard. So the first thing I'm going to do is scale that AKS cluster that we just created up to three nodes, and I can do that with the az aks scale command, specifying the resource group and name of my cluster and the desired node count. And again, this will take several minutes to complete. But once it's done, I can use the kubectl get nodes command again, and we can see the three nodes in our cluster. So now I'm going to deploy another application to our cluster, and it's actually going to be the same voting application that we deployed to Service Fabric in our last module. You might remember that it was a five-container application that let you vote for dogs or cats and view the results of that vote. And in the last module, we deployed a version of this application that used Windows containers, but in this same example-voting-app GitHub repository, there's a Kubernetes deployment YAML file that defines the same application, but using Linux containers. And it's very similar to the structure of the Windows version, apart from that it uses Redis and Postgres containers instead of NATS and TiDB. And here I'm showing you the kube- deployment.yml file from GitHub, which is quite long, and we're not going to look at it in detail, but in here there are definitions of the vote and result websites, the worker container, and the Redis and Postgres containers. And I'll be including this file in the course download materials if you'd like to examine it in more detail. But the great thing is that with just one command, we can deploy this entire five-microservice application to our three-node Kubernetes cluster. So we just say kubectl apply and specify that YAML file, which I've got locally as example- vote.yml. And we can see that it's created five deployments and five services. You might also notice that the Postgres container has got a persistent volume, which is another resource that you can configure with Kubernetes. And now we need to wait for both the vote and result services to be ready, and let's do that with kubectl get service again with the --watch flag. And because I didn't specify a service in particular, it will show us all the services. And as you can see from this output, we're actually running not just this application, but the application from our previous demo on this single Kubernetes cluster. And that's one of the great things about our Kubernetes cluster. You can easily use it to host multiple applications. And this is when namespaces also come in handy, because they would allow us to isolate these applications from one another rather than just showing them all in one pool like we've got here. Now once the external IP addresses are available for the vote and the result services, which we can see they now are, we can visit the vote service IP address on port 5000, and we can see that our cats and dogs voting application has loaded successfully, and so I'll cast my vote again. And now let's also visit the result service on its external IP address on port 5001 to see the results of our vote. And of course, there's only been one vote so far. But next, let's see how we can scale up one of the services in this application to multiple replicas. I've created a second version of the example-vote YAML file with a couple of minor changes to the vote deployment definition. First, I've set the number of replicas to 3, and second, I've defined a couple of environment variables that mean we're now going to be voting for eggs or bacon instead of cats or dogs. And to deploy this updated version, we simply use the kubectl apply command again, pointing at the new YAML file. And you'll notice in the output that for most of the services and deployments it didn't need to do anything as their configuration is unchanged. But the vote deployment configuration did change, and so that's been updated. Now if I want to check whether all three replicas of our voting service are available, I can use the kubectl get deployments vote command, and we can see that there are now three replicas available. And so if I now visit the voting website again in my browser, we can see that we're now voting for eggs or bacon. You'll also notice that this web page is helpfully showing the container ID that processed the request. And so if I refresh the page a couple of times, we should see this container change as different replicas behind the load balancer are handling the request. Okay, before we wrap up this demo, there's one last thing I want to quickly show you, and that's how we can launch the Kubernetes dashboard, which is a really helpful website allowing you to explore all the resources in your Kubernetes cluster. And we can launch it with the az aks browse command, specifying the resource group and name of our cluster. And this will actually create a local proxy website on localhost that securely proxies us through to the dashboard running on the Kubernetes cluster. And if we visit this in our browser, we can see some CPU and memory usage graphs. We can look at the nodes in our cluster and see that there are indeed three of them. We can look at the deployments on the cluster. We can drill down to the level of Pods, and we can also look at the services running on our cluster. And there's loads more to explore in here. You can even scale deployments to multiple replicas directly from within this dashboard. Now, by the way, when I ran this dashboard for the first time on this cluster, I did actually run into some permissions problems, which I had to fix using the following kubectl create clusterrolebinding command for the kubernetes-dashboard. And so I will include this command in the course download materials in case you run into the same problem and need to use it as well, but hopefully this issue will have been fixed by the time you try this out.

Choosing an Orchestrator
Now we're nearly at the end of this course, but I did want to take a moment to discuss how you might choose between the two orchestrators that we've looked at in this course: Azure Kubernetes Service, which we've looked at in this module, and Azure Service Fabric that we looked at in the last module. Now, the first thing to say is that there's not really a wrong choice here. Both are excellent orchestrators and provide solutions to all the problems that orchestrators are designed to help you with, such as container scheduling and upgrades, health monitoring, and service discovery. But each of these services does have a few unique features that might help you make the choice one way or the other, so let me just highlight a few of the key differentiators. Starting with Service Fabric, it's particularly strong at hosting Windows containers, whereas Windows support is something that's still only just being added to AKS at the time of recording. Service Fabric also offers the innovative stateful services programming model that you might want to take advantage of for some of the services in your application. And Service Fabric Mesh brings a very unique approach of an entirely serverless platform where you don't need to provision any nodes in your cluster upfront. One of the biggest selling points of Azure Kubernetes Service is the fact that Kubernetes is the leading open-source container orchestration platform, and that means you get to tap into a huge ecosystem of added value tools and resources. We've already mentioned the Helm package manager, but there's also the Draft open-source tool that simplifies the process of developing applications with Kubernetes. And because Kubernetes has got such broad industry-wide adoption, all of the major cloud providers offer fully managed Kubernetes services with very similar capabilities to AKS, and this greatly reduces the amount of work involved if you wanted to host your application in another cloud. AKS also integrates really nicely with Azure Container Instances, courtesy of the Virtual Kubelet project, which allows you to elastically scale your cluster to handle bursts of load very rapidly while relying on cheaper pre-provisioned nodes for the majority of your day-to-day workloads. And we also mentioned Azure Dev Spaces earlier, which allows developers to run and debug applications directly on an AKS cluster from within developer tools like Visual Studio or Visual Studio Code. So the choice between these is really about which of these added-value benefits are the most appealing and relevant for the projects that you're working on.

Module Summary
[Autogenerated] in this module, we learned about the Azure Kubernetes service, which is a managed kubernetes cluster. That makes it really easy to get your containerized micro service applications up and running. We learned about some key kubernetes concepts, including pods, deployments and service is as well as the coop kuttel, commode, line tolling. But we didn't have time to go into a lot of depth on these concepts. So do check out some of the other courses here on plural site that explore kubernetes in a lot more detail. We saw that we can create an A K s cluster in the portal or with the ese es que es create azure sea ally command. We saw how to connect to the A ks cluster with coop kuttel and used coop cattle apply to deploy some micro service applications defined in yeah mo files. We also saw that we can scale the cluster and scale The number of replicas of individual micro service is in our applications again. In a short module like this, we could only scratch the surface of the capabilities of azure kubernetes service. So I do recommend that you check out this a ks big picture course, which is also available here on floor with sight. If you'd like to go into more depth on a K s in the next module, we're going to look at various recommendations and best practices for securing your containers in Azure.

Securing Containers
Introducing Container Security
In this course, I've shown you four different ways of hosting your containers in Azure. But of course, if you're going to run your applications in the public cloud, it's really important that you know how to ensure that those containerized applications are secure. I'm Mark Heath, and in this module, we're going to look at some best practices and recommendations for how you can secure your containers in Azure. Let's look in a bit more detail at what we'll be learning in this module. We'll start off by looking at how we can secure container images. How can we ensure that the images we're using aren't compromised and don't contain any exploitable vulnerabilities? We'll look at how we can secure their container host. How can we ensure that the computers running our containers are safe from being compromised by an attacker? We'll think about how to manage container privileges. What sorts of things is our container allowed to do? And if it needs to access other resources, like a database, or get hold of secrets, how can we grant it permission to do that? We'll talk about how to secure container networking. How do we expose network access only to the containers that we want to make public whilst locking down network access to other containers so that they can only be accessed by trusted callers? And we'll talk about how we can monitor containers. This is another important part of the security story, as we need to know what's going on with our containers in order to rapidly detect any attacks or security breaches. And as we go through these topics, I'll be highlighting a number of recommended best practices, and many of these will apply whichever Azure service you choose to host your containers on. But we will be focusing particularly on Azure Kubernetes Service in this module, because that's typically platform you choose to host a modern containerized microservices application in Azure, and the complexity of a microservices architecture does introduce some additional security challenges.

Securing Container Images
Let's begin by looking at how we can secure the container images that we're using. When you deploy a containerized application to Azure, you're often using third-party container images, for example, Redis or WordPress. And these container images are typically downloaded from an external container registry. And of course, you're probably also using your own custom container images to host applications you've written yourself, maybe .NET Core or Node.js. And these are typically stored in your own private container registry. It's really important that you can trust the contents of the container images that you're using. We want to be sure that they're free from malware and from vulnerabilities. For third-party container images, as well as for the base images that you're using to build your own containers from, I strongly recommend that you source them from the official registries. For example, if you want to use Elasticsearch, then you should source your container images from the Elastic Docker registry. And here's how you'd reference the Elasticsearch 7.1 .1 Docker image. Or if you're building .NET Core application, then you should source the base images from the Microsoft Container Registry. And here's how you'd reference the ASP.NET Core 2.1 Runtime Docker image. By sourcing your images from the official container registries, recommended by the companies who are producing the software, that gives you much higher confidence that those images have not been tampered with. You should be particularly cautious about sourcing your images from Docker Hub because it's a public registry, and that means anyone can upload an image there. There's no guarantees about the quality and integrity of what's been uploaded by members of the general public. For example, here on Docker Hub, there's a Windows Nano Server version of an Elasticsearch image, and this would be great if you wanted to host Elasticsearch on a Windows Server, but it's not an official build. This has been contributed by a member of the community called sixeyed. Now, I happen to know who this is. This image has been produced by Elton Stoneman, who's a Pluralsight author, and currently works for Docker. And because I know and trust Elton, then maybe I would be willing to use this particular Docker image for my own projects. But as a general rule, do be cautious about adopting images on Docker Hub that have been uploaded by members of the community. You don't know how much care they've taken to ensure that the contents of the image are secure. So it's always better to source images from the official container registries, where possible. What about when you build your own Azure container images? Well, we've already said that in your Dockerfile you should ensure that you only use trusted official images as your base images, but you should also try to use as lean base images as possible. In other words, don't depend on images that have got more features than you actually need. For example, instead of depending on a Ubuntu container image, maybe you can depend on the much more stripped down Alpine or CoreOS images. And this helps you minimize your attack surface area. You may want to consider introducing a base image approval process. Rather than allowing your developers to just build on top of any container image they choose, you could introduce a process where all the containers in use in your application needs to be vetted first. You should be building the container images that you use in production on a trusted Continuous Integration server that itself has been scanned for malware. And once you've built a container image, again, it's recommended that you scan that for malware and vulnerabilities. And there's a number of commercial container image scanning services that you can use for this, such as Twistlock or Aqua. You should store your custom container images in a secure container registry. Azure Container Registry is ideal for this as it's a private registry, and it allows you to host your images in the same datacenters that will be running the code, which is good for performance as the images can download to the hosts very quickly. Obviously, the credentials for pushing images to the registry need to be carefully guarded to prevent an attacker from being able to upload compromised container images to your container registry. There's a premium pricing tier for Azure Container Registry that offers additional security features. It allows you to configure content trust where image is assigned with a cryptographic key, enabling them to be validated as having come from a trusted source. And it also allows you to restrict network access with firewall rules, so you could set it up to only allow access from a specific virtual network. Just because a container has passed a vulnerability scan when you initially created it doesn't mean that it's going to be secure indefinitely. New vulnerabilities are discovered regularly, so you need to have a process in place for keeping your container images up to date. The recommended pattern to use with Docker is not to try apply security patches to existing containers that are running in production. Instead, you replace those containers with an updated version running the new, patched, and secure version of your container. And this means that your CICD process needs to be able to be triggered to create a new image, not just if your source code has been updated, but also if the base image for your container has been updated with a security patch. And it's possible to set up automatic container scanning software that will scan containers in your registry and report any new known vulnerabilities. And then you can use that information to identify which containers running in production need to be replaced. Azure Container Registry has a nice feature called ACR Tasks where it's actually able to perform container builds itself. And these builds can be triggered by a commit to a Git repository or by an update to a base image that your images are using. And this is a great way to automate the process of updating container images to keep them up to date with the latest security patches.

Securing Access to the Container Host
The container host is the computer that's running your Docker containers. So let's consider how we can secure the container host. In this course, we've looked at four different Azure services that can host containers. These were Azure Container Instances, Azure Web App for Containers, Azure Service Fabric, and Azure Kubernetes Service. And those aren't the only ways to run containers in Azure. You could, for example, just install Docker on a virtual machine and use that to host containers. But whichever you choose, it's really important that you secure the container host. For example, the host itself should be patched with the latest operating system updates and the latest version of the container runtime. The good news is that for the four Azure services we discussed in this course, they already do a fair bit of the hard work for you. The underlying virtual machines that are implementing those services are going to be kept patched and up to date automatically without your intervention required. However, if you're using Azure Kubernetes Service, then the version of Kubernetes that your cluster runs is not going to be automatically upgraded. It's actually up to you to choose at what time you want to upgrade to a new version. And Azure Kubernetes Service is able to help you upgrade to a new version of Kubernetes because it's not necessarily that straightforward to do. It adds an extra node your cluster, and then each existing node is cordoned and drained in turn before being upgraded to the new version. And if you're using Service Fabric, a similar process exists for ensuring that your Service Fabric cluster is running the latest fabric version. And there is an option where you can set up your Azure Service Fabric cluster to automatically perform fabric upgrades. The credentials for administration of your container host should also be protected carefully. In Azure, this involves configuring role-based access control, RBAC, to restrict any users of your subscription to only be allowed to perform the actions that they need to. And this lets you choose which users have specific permissions, such as the ability to deploy new containers or restart containers, or view logs, or execute diagnostic commands against containers. Ideally, you want to grant your operations and support staff enough privileges to be able to diagnose and resolve problems in production, but restrict them from being able to access private or sensitive customer data. If you're using Azure Kubernetes Service, then Kubernetes has its own management API and dashboard. And it's also got its own role-based access control that can be used to govern what actions individual users are allowed to perform. Azure Kubernetes Service allows you to integrate with Azure Active Directory to authenticate users of the Kubernetes API. You can then configure Kubernetes role-based access control for those users to control what Kubernetes resources those users have permissions to access and modify. And you do this by defining a role that allows you to grant permissions within a Kubernetes namespace. Now, this is quite a big and complex topic. And in this course, we're not going to do a deep dive into all the Kubernetes security options. But if you are planning to host your containerized applications on AKS, then I recommend that you read this article on the official AKS documentation site, which outlines some of the best security practices. And the first two it mentions here are Authenticate your AKS cluster users with Azure Active Directory, and Control access to resources with role-based access controls. And these go into more detail on the two techniques that we've just mentioned. And there's another best practice it recommends here, use a managed identity to authenticate with other services. And we'll be returning to discuss managed identities shortly.

Securing Container Privileges
When you run software in a container, it's very likely that you'll need to connect to some external resources. For example, you might need to access a database or post a message to a service bus. How can we securely grant our containers the permissions that they need in order to fulfill their tasks? Well, the first thing that we need to say here is that you should apply the principle of least privilege. Only grant your container the minimum privileges it needs to perform the task at hand. For example, this might include not running as the root user in your Linux container, and that gives you additional security against a kernel exploit. But it also means not giving containers access to master keys for sensitive resources like databases. If your container only needs to query from one specific database table, then a good practice is to create a database user with read only access to that single table, and then the container can use that identity to authenticate with the database. But how can we allow our containers to communicate securely with external resources? This usually requires providing your container with access to secrets. These might be connection strings for a database, or API keys for calling cognitive services, for example. And a common way to surface those secrets to the container is to make them available as environment variables. These can be set up at deployment time, and all of the Azure container hosting services that we've looked at in this course support doing that. For example, we saw that when we create an Azure Container Instance, using the Azure CLI, we can provide environment variables with the -e argument. And if we're using Web App for Containers, then we can put our connection strings or secrets into application settings, which will be accessible in the code as environment variables. But there's actually some better patterns for managing the secrets in our containers, so let's look at a few additional options. Azure Key Vault is an Azure service designed to securely store sensitive information, like secrets, API keys, and certificates. If you store secrets in Key Vault, they can be accessed at deployment time and made available to the container, or you could grant your container access to the Key Vault, and this would allow it to fetch the secrets on demand. And there's quite a lot of different ways you can set this up, and it does depend on which of the container host options that we've discussed you're using. If you're using Azure Container Instances, they offer the concept of a secret volume, which is a RAM-backed volume. And this means you can retrieve a secret from Key Vault and store it as a file in the secret volume without ever being written to non-volatile storage. If you're using Web App for Containers, you can use a new feature called Key Vault references. This allows you to add an application setting using a special syntax that looks like this. And when you do this, App Service will automatically access Key Vault for you and replace the value of your application setting with the secret from Key Vault. If you're using Service Fabric, then that's got its own secret management feature based on an encryption certificate. You can encrypt secrets using the certificate, and that makes them safe to store in configuration files. And then the containers running in your Service Fabric cluster can use the certificates to decrypt the secrets. Finally, Kubernetes has also got support for secrets. Secrets are first-class resources in Kubernetes that can be created and managed with the Kubernetes API, or the kubectl command line tool. And the values in a Kubernetes secret are not persisted to disk, but a stored in RAM. And of course, you could still load secrets that are stored in an Azure Key Vault into a Kubernetes secret, either at deployment time or at runtime. Now, you might be wondering, how can I grant my container access to Key Vault without it needing another secret that it uses to authenticate with Key Vault? And that's a great question. And the answer is that we need to make use of what's called managed identities. In Azure, many services give you the ability to create managed identities. And when you set one up, a service principle is automatically created in Azure Active Directory. Or if you prefer, you can choose to use an existing service principle that you've already created. That service principle is then associated with your container host, and so your container is able to use the service principle to authenticate itself with any other Azure resources it needs to access. And that could be Key Vault, but it could also be Service Bus or SQL Database. And there are three main steps in using managed identities. First, you assign a managed identity for the service that's hosting your container. And you can easily enable managed identities for an Azure Container Instance's container group, and an Azure Web App for Containers web app. If you're using Service Fabric, then it's possible to configure a managed identity for the virtual machine scale set that your cluster runs on. Although if you do this, you need to remember that generally it's a good idea to control permissions more granularly than at the cluster level because each containerized service in a microservices application is likely to have its own unique set of resources that it needs to be able to access. If you're using Azure Kubernetes Service, there's an open source project called AAD Pod Identity designed to help you assign managed identities to individual Kubernetes pods. And it does this by deploying two components known as the Node Management Identity server and the Managed Identity Controller to your Kubernetes cluster. Second, you grant the service principle associated with the managed identity permissions to access any resources it needs to, such as Key Vault or a database. Again, make sure you apply the principle of least privilege here. Only grant your service principle the minimum permissions it needs. Third, your container is now able to get hold of an access token for its managed identity by making a web request to a special endpoint that's only available on local host. It can then use that token to authenticate with the Azure resources that it needs to access. Now, I know that possibly sounds a little bit confusing, and there are a lot of ways that you can configure managed identities. But typically, you'll find that once you've enabled managed identities for your container host, using them is very simple. There's many Azure SDKs for different languages including helper methods that help you to get hold of the access token for your managed identity.

Securing Container Networking
It's very common to use containers to host a web applications or APIs that expose network access by report. How can we secure these containers to only allow incoming traffic from trusted sources? The first thing to point out is that Docker containers are locked down by default. You have to explicitly expose a port for it to be able to receive incoming network traffic. And if your container is running as a port in a Kubernetes cluster, you'll also need to configure an ingress controller, which will allow incoming traffic from outside your cluster to be routed to your container. Earlier in this course, when we hosted a ghost blog on Azure Container Instances, we did so by exposing port 2368, and we also gave our ACI container a public IP address and a friendly domain name prefix, and that meant we were able to connect to it from anywhere on the internet. However, that wouldn't be a great state to leave that application in. For one thing, when we're hosting a website, we want to be using HTTPS. And if this was a website that wasn't intended for the general public to access, maybe it was private just for a particular company, then we might also want to restrict incoming traffic to only come from whitelisted IP addresses, or only from a specific virtual network. Earlier in this course, we saw a demo of a five-container microservices application running on AKS. And this had two websites that we wanted to be publicly accessible from outside the cluster, the vote website we wanted everyone to be able to access, and the result website that shows you how many people voted for which option, maybe we'd want to lock that down to only be accessible to administrators. Now in this application, the database and queue containers also had ports exposed, but we don't want them to be accessible at all outside our Kubernetes cluster. We only want the other containers in our application to be able to communicate with them. And that's why we defined only the vote and result containers as Kubernetes services. This meant that they were exposed publicly from the cluster, but the queue and database containers could not be accessed from the outside. Let's quickly go through each of the four container hosting services that we've been looking at in this course and see what help they offer us with securing network access to our containers. First, with Azure Container Instances, you can choose whether your container group has a public IP address or not, so you don't have to expose it publicly. If you want to, you can deploy your container group into a virtual network, allowing you to restrict access to it using standard Azure networking features. Deploying ACI container groups into a VNet is currently in preview at the time I'm recording this, and there are a few limitations to this feature. But hopefully it will become generally available in the near future. With Azure Web App for Containers, of course it's expected that your container is exposing a port for incoming web traffic. And the platform expects your container to be listening on port 80, although you can configure that port number with the WEBSITES_PORT application setting. Then from the outside, you can call your web app either on port 80 or 443, HTTPS termination is handled for you by App Service. And then that traffic is forwarded to your container on the port that you configured. And that's nice because it means your container itself doesn't need to implement HTTPS. You can optionally provide your own custom domain name and custom SSL certificates. And another App Service feature that you can use is defining access restriction rules, which enable you to whitelist incoming IP addresses, or even lock down incoming web traffic so that it has to come from a specific subnet on a virtual network. Another security related App Service feature is that you can enforce authentication for the entire website. And this requires all incoming traffic to have a valid bearer token from a third-party identification provider. And several of these are supported including Azure Active Directory, Google, Facebook, and Twitter. And if any incoming request isn't authenticated, then you get redirected to log in before you reach your web application. Azure Service Fabric provides a lot of flexibility for configuring networking just the way you want to. An Azure Service Fabric cluster runs in a virtual machine scale set, which can be installed into an Azure virtual network. And this means that you can take advantage of all the virtual network security features such as network security groups, or NSGs. And you can set up a static public IP address and load balancers to direct incoming traffic. And if you are familiar with working with these low-level networking constructs, they can be configured to give your containerized Service Fabric application the same high levels of security that you can achieve with the traditional IaaS set up consisting of virtual machines on a virtual network.

Securing Container Networking in AKS
How can we secure container networking when we're using Azure Kubernetes Service? AKS offers a lot of flexibility for configuring your networking. Not only does it integrate really well with other Azure services, but Kubernetes itself also offers a wealth of networking-related features. Kubernetes comes with a concept of ingress controllers and resources, which can be used to route incoming web traffic to the appropriate containers running in your cluster. There's two modes of networking that you can use with Kubernetes. There's the kubenet networking, and Azure CNI networking, in which pods receive individual IP addresses on a virtual network. And this best practices document from Microsoft recommends that you choose CNI networking for most production scenarios, which requires you to put the AKS cluster onto a virtual network. Kubenet networking doesn't require you to create a virtual network yourself, meaning that it might be a bit simpler for development or test clusters, but for a production AKS cluster, the recommendation is that you consider CNI networking. Now, all of the four Azure container hosting services that we've discussed can be set up so that incoming traffic from the public internet can directly reach that container host. But perhaps with the exception of Web App for Containers, which is intended for hosting public-facing containerized web applications, a very helpful pattern is to put a separate Azure service in front of your container host and let that service handle incoming traffic and pass it on to your container host. And Azure Application Gateway is ideal for this task. It receives incoming traffic from the public internet and forwards it onto your containers, which can be hosted securely in a virtual network. So let's look at an example scenario. This diagram from the best practices section of the AKS documentation shows an AKS cluster being hosted within a virtual network with a private IP address and an internal load balancer. And this Kubernetes cluster is not accessible at all from the public internet. But here, an Azure Application Gateway is deployed into another VNet, which does expose a public endpoint to the internet. This application gateway can have a web application firewall, or WAF, configured, which can scan incoming traffic for attacks, including many of the top OWASP vulnerabilities. VNet peering is then configured to allow the gateway to forward traffic on securely to the Kubernetes cluster. And you could use this exact same setup for a Service Fabric cluster or for Azure Container Instances with those containers secured on a virtual network. We focused a lot so far on how traffic gets into your containers from the outside, but if you're running a microservices application on Azure Kubernetes Service, how can you control which containers are allowed to talk to each other internally? Ideally, we should be locking down the containers inside our cluster so that they can only communicate with the other containers that they're supposed to, and doing that would limit the damage that any rogue code that somehow managed to run inside your cluster was able to do. And there are a few approaches to locking down internal networking communication within your cluster. One is to make use of Kubernetes network policies. These control what traffic is allowed to fly between pods. In the AKS best practices documentation page that we looked at earlier, it gives this simple example of a Kubernetes network policy that selects pods with the app: backend label applied to them, and then defines an ingress role that ensures that those pods can only receive incoming traffic from pods with the app: frontend label. Another approach to controlling traffic within a cluster is to install something known as a service mesh, which is not to be confused with Azure Service Fabric mesh, that's a completely different thing. Two very popular open source service meshes are Istio and Linkerd. When you install a service mesh into your Kubernetes cluster, an extra container, sometimes known as a sidecar container, is deployed alongside every one of the containers in your application. So a service mesh container sits inside every Kubernetes pod. Then, all communication between your services is handled by the service mesh, and this provides several benefits. The service mesh can encrypt all traffic in transit without you needing to configure anything. It can also enforce rules about which container is allowed to talk to which other container. And it can also handle networking retries, as well as give you much more detailed observability of what's going on in your cluster. Service meshes provide much more than just security, but additional security is one of their many benefits. Of course, introducing a service mesh does introduce a bit more complexity into your overall system architecture. Finally, it's also worth mentioning that a recommended best practice is to avoid using a shared Kubernetes cluster for both your development and testing and production workloads. Although Kubernetes supports the isolation of different applications running on the same cluster by means of namespaces, you'll have much better security if your production cluster is completely separate. Doing this will not only help you avoid accidentally allowing testing workloads to destabilize your production system, but it also means that the production cluster can be much more tightly locked down and access only granted to those people with responsibility for operations.

Monitoring Containers
The last aspect of security I want to briefly touch on is the value of good monitoring of our containers. If there's a security breach, we want to be able to detect it as soon as possible, and this means having good diagnostics about what our containers are doing, where traffic is coming from, how much CPU they're utilizing, and more. If we can monitor these sorts of thing, then we can configure alerts that will warn us when things are happening out of the ordinary in our containers. The good news is that all of the container hosting services in Azure integrate with Azure Monitor to capture logging and telemetry information. And from within Azure Monitor, we can not only query this diagnostic information, but set up alerts based on it. You can also capture the container log output and send it to Azure Log Analytics. But do bear in mind sometimes container log output can contain sensitive customer information, so you might want to restrict who has access to view those logs. It's also important that all administrative access to our containers is audited. Azure supports auditing all deployment actions performed through the Azure Resource Manager. And you can also configure the Kubernetes API to audit all deployments to your AKS cluster. And you should also consider making use of Azure Security Center. This is a service in the Azure portal that will automatically inspect all of your Azure resources and warn you about security issues with the way that you've configured things, particularly when you fail to follow best security practices. And this has recently been expanded to include checking the configuration of container hosting services, such as Azure Kubernetes Service.

Module Summary
Let's recap the container security best practices and guidelines that we've learned about in this module. Your container images should come from trusted and official container registries, and you should scan your images for malware and vulnerabilities. You should have a plan and preferably an automated process to update your containers running in production whenever a new security patch for a vulnerability becomes available. You should carefully guard who has access to the container host by using Azure role-based access control and applying the principle of least privilege. If you're using AKS, then you should also restrict who has access to the Kubernetes API for your cluster with Kubernetes role-based access control. Your container should only be given the secrets that it absolutely needs to do its job, and the credentials you give it to access external resources should also only provide the privileges that are required. Any secrets that your containers access should be stored using the security mechanisms provided by your container host. And some of the options we discussed include using Kubernetes secrets and Azure Key Vault. If possible, make use of managed identities to reduce the number of secrets that your containers need access to. Any containers that expose a port should only allow incoming traffic from trusted callers. One good option is to place your container host, such as AKS, in an Azure virtual network and use Azure networking features like NSGs, application gateways, and web application firewalls to control who has access and protect you from attackers. If you're using Kubernetes, you can also make use of ingress controllers, network policies, and even service meshes to provide fine-grained control over what containers can talk to each other. Finally, ensure that you're capturing sufficient logging, telemetry, metrics, and auditing information to enable you to detect when attacks are happening and to understand what has happened in the event of a security breach. And that brings us to the end of this course. All of the scripts, configuration files, and demo applications for this course are available in the course down materials on Pluralsight. And they're also available here on GitHub with a few extra bonus scenarios included. And I'll try to keep this GitHub repository updated with any changes to the syntax of the commands or configuration files. I hope you've enjoyed this journey of learning a bit about Docker and the various ways that we can run containers in Azure. Feel free to contact me with any feedback or questions you have, and thanks for watching.